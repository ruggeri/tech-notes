\section{Basics}

\subsection{Notation}

\begin{remark}
  In classical computing, machines apply deterministic operations on
  deterministic registers. The registers store bits that have a value of
  exactly zero or one. If the machine has $k$ bits of memory, then a
  natural way to represent the current state of the machine is as a
  bitstring of zeroes and ones in the space $\setof{0, 1}^k$, which has
  $2^k$ distinct values.
\end{remark}

\begin{remark}
  A less natural way to view the state of the machine is as a bitstring
  of length $2^k$ with \emph{exactly one bit set to one}.
\end{remark}

\begin{example}
  A computer with three bits of memory can be in any one of $2^3 = 8$
  states. If the first and third bits are set to one, and the second is
  set to zero, we could ``naturally'' represent the machine state as the
  vector $\parens{1, 0 , 1}\tran$. However, I prefer to represent the
  state as $\parens{0, 0, 0, 0, 0, 1, 0, 0}\tran$. Since this is a long
  representation, I will write this long form as $\braket{101}$. Note
  that $\braket{101}$ is an 8-dimensional vector, while $\parens{1, 0,
  1}\tran$ is a 3-dimensional vector.

  Just to keep things weird, I'll also write $\braket{5}_3$, which is
  synonymous with $\braket{101}$.
\end{example}

\begin{remark}
  Why write the state as a $2^k$ dimensional vector? A deterministic
  machine must have exactly one position set to exactly one, so the use
  of $2^k$ bits in the representation is redundant.

  Imagine a three bit \define{probabilistic} machine. Imagine the
  machine is configured such that it is in a ``superposition'' of two
  states. If the machine is forced to ``collapse'' its state, there is a
  50/50 chance that the machine will be found to be in state 000 and
  state 101. We may represent this state as:

  \begin{nedqn}
    \begin{bmatrix}
      0.5 \\ 0 \\ 0 \\ 0 \\
      0 \\ 0.5 \\ 0 \\0
    \end{bmatrix}
  \end{nedqn}

  \noindent
  We may write this as: $0.5 \braket{000} + 0.5 \braket{101}$.
  Equivalently: $0.5 \braket{0}_3 + 0.5 \braket{5}_3$.
\end{remark}

\begin{remark}
  The ``natural'' representation of length-3 bitstrings is insufficient
  to represent such a superposition. If we wrote $\parens{0.5, 0,
  0.5}\tran$, would that not imply a 25\% chance of observing $000$, a
  25\% chance of observing $001$, a 25\% chance of observing $100$, and
  a 25\% chance of observing $101$?

  Only the $2^k$ dimensional way allows us to have ``entangled'' bits.
\end{remark}

\begin{remark}
  Let's take a step back from the probabilistic and consider only a
  deterministic machine. Let's define an \define{operation} of the
  deterministic machine. I say that an operation takes $m$ bits of state
  and outputs $n$ bits of state. A deterministic operation is a function
  defined for each of $2^m$ possible input bitstrings, outputting $n$
  bit values. Even though a deterministic operation need only specify
  $2^m n$ bit values, we will encode the output in our usual way. Thus
  an operation is represented as a $2^n$-by-$2^m$ matrix.
\end{remark}

\begin{example}
  Here is the
  NOT operation:

  \begin{nedqn}
    \begin{bmatrix}
      0 & 1 \\
      1 & 0 \\
    \end{bmatrix}
  \end{nedqn}

  \noindent
  Let's apply it to the state $\braket{1}$:

  \begin{nedqn}
    \begin{bmatrix}
      0 & 1 \\
      1 & 0 \\
    \end{bmatrix}
    \braket{1}
  \eqcol
    \begin{bmatrix}
      0 & 1 \\
      1 & 0 \\
    \end{bmatrix}
    \begin{bmatrix}
      0 \\ 1
    \end{bmatrix}
  \\
  \eqcol
    \begin{bmatrix}
      1 \\ 0
    \end{bmatrix}
  =
    \braket{0}
  \end{nedqn}
\end{example}

\begin{example}
  Here is the matrix representing the AND operation:

  \begin{nedqn}
    \begin{bmatrix}
      1 & 1 & 1 & 0 \\
      0 & 0 & 0 & 1
    \end{bmatrix}
  \end{nedqn}

  Lets apply the AND operation to the state $\braket{11}$:

  \begin{nedqn}
    \begin{bmatrix}
      1 & 1 & 1 & 0 \\
      0 & 0 & 0 & 1
    \end{bmatrix}
    \braket{11}
  \eqcol
    \begin{bmatrix}
      1 & 1 & 1 & 0 \\
      0 & 0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      0 \\ 0 \\ 0 \\ 1
    \end{bmatrix}
  \\
  \eqcol
    \begin{bmatrix}
      0 \\ 1
    \end{bmatrix}
  =
    \braket{1}
  \end{nedqn}

\end{example}

\begin{remark}
  We may summarize: deterministic operations that map $m$ input bits to
  $n$ output bits are represented as $2^n$-by-$2^m$ matrices where there
  is exactly one $1$ value per column. These are sometimes called
  \define{deterministic transition matrices}.
\end{remark}

\begin{remark}
  An operation is also called a \define{gate}. Any deterministic
  transition matrix can be constructed through the appropriate
  successive applications of NOT and AND gates. Or NOT and OR gates. Or
  simply just NAND (NOT AND) gates!

  This is to say: if we are allowed to successively apply NOT and AND
  gates, we can construct any desired Boolean operation (also called a
  \define{circuit}).
\end{remark}

\begin{remark}
  Boolean circuits map a fixed, finite number of bits to a fixed, finite
  number of bits. The notion of \define{algorithm} goes (just a little)
  further than this. An algorithm defines (1) how to choose the next
  input bits from the current state, (2) how to choose the next
  operation to perform, (3) where to store the output bits in the
  current state, and (4) when to halt and what to output as the final
  result.
\end{remark}

\begin{remark}
  We get Turing completeness as soon as we extend the notion of a fixed,
  finite series of operations with a conditional jump operation.
\end{remark}

\begin{remark}
  Extending our Boolean circuit concept with the algorithm concept lets
  us define procedures that calculate (some) functions $f: \setof{0,
  1}^* \to \setof{0, 1}^*$. These are functions with unbounded (but
  finite) input size and unbounded (but finite) output size.

  Of course, not all functions are computable by our algorithm over
  boolean circuits. For instance, no algorithm over boolean circuits
  will solve the halting problem (map $(x, y)$ to zero iff the program
  represented by $x$ halts when fed $y$ as input).
\end{remark}

\subsection{Probabilistic Computing}

\begin{remark}
  We've defined our deterministic state and operations in such a way
  that it is easily generalized to \define{probalistic} computing. Lets
  start doing that!
\end{remark}

\begin{remark}
  We already talked about how we could represent \define{probabilistic
  state}. For instance: $0.5 \braket{000} + 0.5 \braket{101}$. Or more
  generally for a $k$-bit state:

  \begin{nedqn}
    \sum_{i = 0}^{2^k} p_i \braket{i}_k
  \end{nedqn}

  \noindent
  We call the states $\braket{i}_k$ (of which there are $2^k$)
  \define{basis states}.
\end{remark}

\begin{remark}
  As long as we have probabilistic states, why not have
  \define{probabilistic} gates? For instance, imagine this gate that
  flips a coin to decide whether to leave two input alone, or whether to
  AND them and write the result in the second:

  \begin{nedqn}
    0.5
    \begin{bmatrix}
      1 & 0 & 0 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & 1
    \end{bmatrix}
    +
    0.5
    \begin{bmatrix}
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 0 & 0 \\
      0 & 0 & 0 & 1
    \end{bmatrix}
  \eqcol
    \begin{bmatrix}
      1 & 0 & 0.5 & 0 \\
      0 & 1 & 0   & 0 \\
      0 & 0 & 0.5 & 0 \\
      0 & 0 & 0   & 1
    \end{bmatrix}
  \end{nedqn}

  \noindent
  Non-deterministic transition matrices like this are called
  \define{stochastic} or \define{Markov} matrices.
\end{remark}

\begin{remark}
  I claim that keeping the probabilistic state throughout a computation
  is unnecessary. After each operation, I argue that you should just
  observe the current state (collapsing the working state to a
  deterministic basis state) before proceeding to the next observation.
  There is no need to store superpositions of basis states. That is:

  \begin{nedqn}
    M \sum_{i = 0}^{2^k} \alpha_i \braket{i}_k
  \eqcol
    \sum_{i = 0}^{2^k} \alpha_i M \braket{i}_k
  \end{nedqn}

  \noindent
  Thus the power of probabilistic computing (if any) comes from
  probabilistic gates.

  (In fact: the ``observation'' operation is needed for algorithms since
  we need to observe before doing a conditional jump.)
\end{remark}

\begin{remark}
  In fact, the model with probabilistic gates is equivalent to having
  deterministic gates except for a special no-input gate that takes an
  input and outputs a coin flip.

  I suppose that I should be more careful. Not every discrete
  probability distribution can be constructed from such a gate, right?
  But any discrete distribution can be arbitrarily closely approximated.
\end{remark}

\begin{remark}
  We may ask: have we expanded what it means to be ``computable?''
  Certainly all deterministic programs are also probabilistic programs
  (that don't use any randomness). But can probabilistic programs
  ``compute'' something (anything?) that a deterministic program cannot?

  If we say a machine ``computes'' $f: \setof{0, 1}^* \to \setof{0,
  1}^*$ if the machine \emph{always} produces $f(x)$ when fed $x$, then
  probabilistic computing gives us no additional power.

  That is: if you use a probabilistic machine to deterministically
  compute a function, you didn't actually need any ``intermediate
  randomness'' that you used.

  I argue from simulation of the probabilistic machine. Remember that
  after every probabilistic operation, we can/should simply collapse the
  probabilistic state to one basis state. Eventually the probabilistic
  algorithm should halt. So a deterministic machine can simulate
  (perhaps slowly) a probabilistic machine simply by non-randomly
  collapsing the simulated machine's probabilistic state to the
  lexicographically first basis state with any positive probability on
  it.
\end{remark}

\begin{remark}
  We could be a little more precise. What if the probabilistic machine
  loops forever while a $0.01 \braket{0} + 0.99 \braket{1}$ bit is not
  zero?

  To rectify this problem, we should have the deterministic machine
  simulate, in parallel, all possible results of a series of
  observations. This is similar to showing that NP is in PSPACE and
  EXPTIME.
\end{remark}

\begin{remark}
  We may propose alternate definitions of computability. For instance: a
  function $f$ is probabilistically computable if a probabilistic
  machine, when fed $x$, will produce the correct output $f(x)$ with
  probability greater than the probability of producing any incorrect
  $y'$.

  By repeatedly using the machine and returning the most common result,
  we can achieve a machine that computes the correct $f(x)$ with
  arbitrarily high probability.

  Are the probabilistic machines more powerful in this sense? Again no.
  We may simply do our parallel simulation. Provided the expected
  runtime of the probabilistic algorithm is finite, we will eventually
  simulate 50\%, 75\%, 99\%, of all possible execution paths. Eventually
  we should see that even if \emph{all} remaining execution paths lead
  to the 2nd most popular (wrong) answer, the 1st most popular (right)
  answer will still be in the lead.

  My argument fails if the machine uses an expected infinite number of
  random bits. (Though I think my argument still works when the machine
  merely has an infinite expected runtime!)
\end{remark}

% Source: https://cstheory.stackexchange.com/questions/1263/truly-random-number-generator-turing-computable

\begin{remark}
  Having considered \emph{computability} in the probabilistic paradigm,
  we can next investigate various \define{complexity classes} for
  probabilistic machines. For instance: PP consists of those problems
  where a probabilistic machine, in time polynomial with the size of the
  input, will output the correct answer with probability greater than
  50\%.

  More strict is BPP, which says that the probability of the correct
  answer must always be at least 50.1\%. If so, we can take a 50.1\%
  algorithm and always ``amplify'' it to a 99\% algorithm.

  Amplification doesn't work with PP necessarily, since the probability
  of the correct answer may drop asymptotically toward 50\% fast enough
  that larger problem sizes need more and more trials to amplify to a
  higher probability.
\end{remark}

\begin{remark}
  RP consists of decision problems where a polytime probabilistic
  machine exists that (1) always gives a correct NO answer and (2) gives
  a YES answer with probability at least 50\%.

  Co-RP consists of the ``opposite'' problem: the algorithm always gives
  a correct YES answer but can sometimes give a wrong NO answer.

  ZPP consists of those problems that are in the intersection of RP and
  Co-RP. There exists a polytime algorithm that always returns a correct
  YES/NO answer, except for a less than 50\% chance of returning ``don't
  know.''

  Equivalently: the ZPP algorithm is always correct, and runs in
  expected polytime.
\end{remark}

\begin{remark}
  For a long time, we didn't have a polytime algorithm for primality
  testing. But we had a simple BPP approach based on \define{Fermat
  testing}. We know $x^p = x \pmod{p}$. So keep choosing random $x$
  values, and if you never find a violation, then $p$ is probably a
  prime! If $p$ is not prime, then there are at least $\sqrt{p}$ numbers
  that are not coprime with $p$ and where $x^p \ne x \pmod{p}$.

  This \emph{suggests} that BPP might be larger than P, but we don't
  know. In fact (surprising to me), researchers do expect that P=BPP.

  Since BPP already includes ZPP, we do not expect that a probabilistic
  computer increases the bounds of what is \define{efficiently
  computable} by either of those standards.
\end{remark}

\subsection{Quantum State}

\begin{remark}
  Probabilistic machines operate on states which are \emph{unit vectors}
  with respect to the $L_1$ norm. Markov matrices are exactly those
  matrices that preserve $L_1$ norm.
\end{remark}

\begin{remark}
  The coordinates of a state vector $v$ tell us how to decompose $v$
  into a weighted sum of basis vectors $\sum_i v_i \braket{i}_k$.
  However, this privileges the \define{standard basis} $\braket{i}$.
  What about \emph{rotations} of the original basis?

  If we want to ``decompose'' $v$ into ``rotations'' of an original
  basis, we need to nail down a notion of \define{inner product}.

  But there is exactly one kind of inner product for $\R^n$: the dot
  product. And this is compatible with exactly one kind of norm: the
  $L_2$ norm.
\end{remark}

\begin{remark}
  If you ``rotate'' your state, you should be able to correspondingly
  rotate your reference frame to observe the state. And the only notion
  of measurement that can correspond to the concept of rotation is the
  $L_2$ norm.

  I'm being handwavy.
\end{remark}

\begin{remark}
  So the quantum state is an extension of the notion of probabilistic
  state. Quantum states are unit vectors with respect to the $L_2$ norm.
  Coordinates can have negative, or even complex, values. (Aaronson
  assures us that only negative values are needed for quantum computing
  purposes.)

  The matrices that manipulate quantum states preserve $L_2$ norm. Each
  column consists of a vector with unit $L_2$ norm. Such matrices are
  called \emph{unitary matrices}. When the matrix consists of only real
  values these are \emph{orthogonal matrices} (also called
  \emph{rotation} matrices).

  The unitary matrices are all \emph{invertible}, and their inverses are
  also unitary matrices.
\end{remark}

\begin{remark}
  If we measure a quantum state $x$ relative to an orthogonal basis that
  includes $b$, then the probability of observing $b$ is $\sqrt{x \cdot
  b}$
\end{remark}

\begin{example}
  A 50/50 state in quantum land is $\sqtot
  \parens{\braket{0} + \braket{1}}$. This has norm zero, and each state
  is equiprobable. We can find the probability of measuring a
  $\braket{0}$ in the \define{computational basis} by:

  \begin{nedqn}
    \parensq{
      \sqtot
      \parens{\braket{0} + \braket{1}}
      \cdot
      \braket{0}
    }
  \eqcol
    \parensq{\sqtot}
  =
    \frac{1}{2}
  \end{nedqn}

  There are are many different ways to have a 50/50 variable. There are
  other rotations of $\sqtot \parens{\braket{0} +
  \braket{1}}$, plus there are also flips of coordinates. There are an
  infinite number if you allow complex numbers!
\end{example}

\begin{remark}
  We call the coordinates of a quantum state \define{probability
  amplitudes}.
\end{remark}

\begin{remark}
  Unitary transformations are invertible, thus reversible. The inverse
  is the conjugate transpose of the matrix.
\end{remark}

\begin{remark}
  Observing collapses a quantum state into a basis state. It is
  destructive and not invertible. With ``regular'' probabilistic
  computing, observation of intermediate state could never ``harm'' our
  algorithm. Not so with quantum computing!

  Imagine rotating an unknown state $\braket{0}$ to $\sqtot
  \parens{\braket{0} + \braket{1}}$. If you don't observe, you can
  perform a CCW rotation to deterministically retrieve the original
  state. But if you \emph{do} observe, the same CCW rotation will give
  you either $\sqtot \parens{\braket{0} - \braket{1}}$ or $\sqtot
  \parens{\braket{1} - \braket{0}}$ (depends on whether you observed
  intermediate state of: $\braket{0}$ or $\braket{1}$).
\end{remark}

\begin{remark}
  Note that if you don't observe, and apply a \emph{second} rotation by
  45deg, you'll get the state $\braket{1}$. You've rotated by 90deg!

  Let's examine this:

  \begin{nedqn}
    \begin{bmatrix}
      \sqtot & - \sqtot \\
      \sqtot & \sqtot
    \end{bmatrix}
    \begin{bmatrix}
      \sqtot \\ \sqtot
    \end{bmatrix}
  \eqcol
    \sqtot
    \begin{bmatrix}
      \sqtot \\ \sqtot
    \end{bmatrix}
    +
    \sqtot
    \begin{bmatrix}
      -\sqtot \\ \sqtot
    \end{bmatrix}
  \\
  \eqcol
    \begin{bmatrix}
      \half \\ \half
    \end{bmatrix}
    +
    \begin{bmatrix}
      -\half \\ \half
    \end{bmatrix}
  \\
  \eqcol
    \begin{bmatrix}
      \half - \half \\ \half + \half
    \end{bmatrix}
  =
    \braket{1}
  \end{nedqn}

  \noindent
  Please note the \emph{destructive} interference at state $\braket{0}$
  and the \emph{constructive} interference at state $\braket{1}$. This
  is the glory of quantum computing: applying operations that have
  destructive interference at some basis states (the ``wrong'' answers)
  and constructive interference at other basis states (the ``right''
  answers).
\end{remark}
