## ASCII, Latin-1, Windows-1252

In the beginning there was ASCII.

Then they wanted to add more Latin characters. This is "Latin-1" AKA
ISO 8859-1.

There is a Windows superset of Latin-1 called "Windows-1252" (also
called CP-1252). This includes all the Latin-1 printable characters, but
repurposes some non-printing control characters as additional printable
characters. Historically it has been common for websites to claim a
content type of Latin-1, but include the Windows-1252 characters that
aren't true Latin-1 characters. For this reason, HTML5 standard says
that if a website says that something is in Latin-1, browsers should
treat it as Windows-1252.

## Unicode, Graphemes, Code Points

Unicode then came around. Every "grapheme" gets a "code point." A code
point is simply a number. A series of numbers thus describe a string.
Graphemes are not necessarily characters: they can be ligatures. Thus,
the notion of a "character" can consist of multiple code points in
succession. This is called a "grapheme cluster." So even given a
sequence of code points, it's a parsing problem to get individual
"characters."

The red heart ‚ù§Ô∏è is a good example. It combines two code points: heart
and "colorful." (BTW other colored hearts just have their own code
points; they don't do combining).

For most programming languages, you can't rely that `charAt(idx)` will
give you the *grapheme cluster* at position `idx`. You'll be lucky to
get the *unicode code point* at position `idx`. You may well just get
the *byte* at idx.

For backward compatibility, the ASCII characters get the same Unicode
code points. That is, characters in the range 0-127 are the same
whether ASCII or Unicode.

For now, the highest Unicode codepoints reserved are 21 bits long.

## UTF-32

The next question is how to encode the code points to bytes. The
simplest format is probably UTF-32. It is fixed width, and each code
point is encoded as a 32-bit number. (UTF-32 assumes that max code
point is 21 bit long).

**Byte Order Mark**

Question: should the 32-bit UTF-32 representation be stored
*little-endian* or *big-endian*? This is the purpose of the *byte order
mark* (BOM). The BOM has codepoint U+FEFF. You store the BOM at the
start of a file. If the file starts with the bytes 0000FEFF, you know it
is big-endian. If it starts FFFE0000, then you know it is little-endian.

Note: we're relying on the implicit assumption that U+FFFE is invalid.
Of course, for exactly this reason Unicode declares U+FFFE invalid.

## UTF-8

The most common format is UTF-8. Here, each code point is encoded with
1, 2, 3 or 4 bytes. Code points are stored like so:

  0xxxxxxx (code points 0-127)
  110xxxxx 10xxxxxx (code points requiring two bytes)
  1110xxxx 10xxxxxx 10xxxxxx (code points requiring three bytes...)
  11110xxx 10xxxxxx 10xxxxxx 10xxxxxx

Basically: basic ASCII characters are marked with a leading zero. Higher
code points have their first byte describe how many bytes are needed.
"Continuation bytes" start with "10". This allows for
"self-synchronization" if a byte is dropped in transmission. Else
everything could be garbled by the loss of one byte.

**Byte Order Mark**

There is no need for the BOM in UTF-8. UTF-8 describes how a code point
is crammed into a multi-byte sequence. It is basically big-endian: the
most significant bits get crammed into the first byte, and the less
significant bits in the next.

The Unicode Standard does not recommend storing the BOM in a UTF-8 file.
This would break backward compatibility with ASCII. But it is not
forbidden. Windows apparently does store a BOM. A BOM might represent
that a file is stored in UTF rather than ASCII. But it is recommended to
signal this in other ways...

**Efficiency**

UTF-8 is biased toward Latin characters: ASCII characters have
codepoints in the range 0-127, so they are always 1-byte encoded, and
UTF-8 is the same as ASCII if only encoding ASCII text.

Unicode has restricted itself to 21 bits worth of code points. That
means that all code points can be represented in UTF-8 with at most four
bytes (3bits + 6bits + 6bits + 6bits). This means that UTF-8 is always
as efficient as UTF-32.

If Unicode were to go back on its word and allow more than 21 bits worth
of code points, then sometimes UTF-32 might be more efficient.

## UCS-2 and UTF-16

UCS-2 was a fixed-width two byte format from when we thought we only
needed 2**16 codepoints. Java and JavaScript are prominent users of
UTF-16.

When Unicode added more characters, it broke UCS-2. UTF-16 is the
evolution of UCS-2. It is a variable-width format of two or four bytes.
If you use only characters from the "Basic Multilingual Plane," then
UTF-16 encodes every character with exactly 2 bytes. But for higher code
points, four bytes will be needed.

**Byte Order Mark**

UTF-16 representations consist of either one or two "code units" (16bit
values). The Byte Order Mark describes whether a code unit is stored big
or little endian. So files will either start 0xFEFF (big endian byte
order mark) or the invalid 0xFFFE (little endian byte order mark).

**Efficiency**

UTF-16 is like UTF-8, but uses one or two 16bit quantities. Like UTF-32
it *cannot* be treated like ASCII. It can be more efficient if there
were a lot of 3 byte UTF-8 codes that can now be represented with 2
bytes (less overhead). I think that is the case for CJK text.

## UTF-7

UTF-7 is an interesting case: it is a *7 bit* encoding for
Unicode. The idea is to map every character to a valid ASCII
character. This means that UTF-7 is safe for use with software that
may not be "8-bit clean." There may be software that thinks it can use
the top bit of an ASCII character however it likes. This would garble
a text sent in a format that needed all 8 bits.

## Base64

Another example could be Base64 encoding. This encodes everything to a
*printing* ASCII character. It works in 6bit chunks: it maps every
6bits to a 7bit ASCII character. And each ASCII character is stored
using 8 bits. That means 4:3 increase in size.

Base64 could be used to encode UTF-8, -16, or -32 to be 8-bit
clean. But it can also be used of course for binary files. It is the
common format used to convert binary files to text, for upload in an
HTTP post request.

## Base64, and URLs

Base64 uses some ASCII characters like "+", "=", "/" that aren't safe
for use in URLs. Imagine if you want to use a 128-bit id in a
URL. Then you need to Base64 encode, but you additionally have a
problem that this uses characters not arbitrarily allowed in URLs. So
it is typical to further replace "+", "=", "/" with *percent encoded*
values like %2B, %2F, et cetera.

Alternatively there is the base64url format, which avoids the problem
by replacing using "-" instead of "+" and "_" instead of "/". There is
still the problem with "="...

## uuencoding

`uuencoding` (Unix-to-Unix encoding) is a format which predates Base64.
It is very similar in spirit: it converts into a base 64 representation,
and uses ASCII printing characters. However, it avoids the use of
lowercase letters because in olden days some printers didn't print
those!

## yEnc

In 2001 there came to be another encoding that tried to be as
lightweight as possible but still work with Usenet and email.

252 of 256 byte values are passed through unencoded. It is assumed that
these will not have any special meaning to Usenet or email, and will be
left unmolested. However, NUL, LF, CR, and = are escaped, as these do
have special meaning.

# Python2 and Python3 ASCII and Unicode Strings

**Python2**

Python2 has two string types: `str` and `unicode`. Methods like `len`
aren't going to work if you put some unicode character codes in a
string. For instance, "√î" needs two bytes in UTF-8, and you'll get len 2
of this as an `str`, vs len 1 as a `unicode`. You get that by prefixing
`u"√î"` vs just `"√î"`.

    len("√î") == 2
    len(u"√î") == 1

    "√î" == '\xc3\x94'
    "√î".decode("UTF-8") == u"√î"
    u"√î".encode("UTF-8") == '\xc3\x94'

It appears that Python2 assumes UTF-8 encoding? It's probably more a
question of how the terminal delivers bytes typed from a keyboard to the
interpreter.

**Python3**

In Python3, there is no more `unicode`; `str` is already by default
`unicode`. To replace the old kind of `str`, there is a class called
`bytes`. In Python2, `bytes` was just an alias for `str`.

    len("√î") == 1
    "√î".encode("UTF-8") == b'\xc3\x94'
    b'\xc3\x94'.decode("UTF-8") == "√î"

**Python File Formats**

Python2 will read all files as binary, whether you write "b" or not.
File contents will be read in as an `str` regardless, since `str` is
both how strings are stored and how bytes are stored. If you want to
decode a non-ASCII file to `unicode`, you must do this yourself
explicitly.

On the other hand, Python3 will try to read a `str` (versus a `bytes`)
from a file unless the "b" flag is given. Python3 expects the file to be
in UTF-8 format. This is fine even if the file is in ASCII. But if the
file is in either Latin-1 or Windows-1252, this is not compatible with
UTF-8 and will cause a parsing error.

A few ways you can handle this:

    open("file.txt", "r", encoding="latin-1")

Or you can open as binary:

   bs = open("file.txt", "rb").read()
   s = bs.decode("latin-1")

## JavaScript

JavaScript strings are stored as UTF-16 strings.

As expected, `charAt(idx)` will *not* give you the grapheme cluster at
position `idx`. It won't even give you the code point at position `idx`.
That kinda makes sense: finding a code point requires parsing the
string.

Great. Then surely `charAt(idx)` gives you the *byte* at position `idx`?
Wrong!

Instead, `charAt(idx)` will give you the "code unit" (not "code point")
at position `idx`. A code unit is a 16bit quantity. Recall that UTF-16
is a variable length encoding where most characters are encoded with 16
bits, but higher valued code points are encoded with 32 bits.

Basically: JavaScript must come from the time of UCS-2. In UCS-2, it was
assumed that all Unicode code points would fit in a 16bit fixed width
encoding. When that was the case, `charAt` did exactly the right thing.
But now that strings are encoded in variable-length UTF-16, `charAt`
will sometimes "go inside" a 2-unit Unicode character and grab one of
the code units.

**Example**

This happens with the fire emoji. The fire emoji has a code point
0x1F525 which is 128293 in decimal. This is greater than 2**16 so
naturally this requires two code units in UTF-16.

Let me give an example in Python:

    len("üî•") == 1
    x = "üî•".encode("UTF-16le")
    x == b'=\xd8%\xdd'
    len(x) == 4

Note that I chose to use UTF-16le. Look what would happen if I just used
UTF-16 without specifying the byte order:

    x = "üî•".encode("UTF-16")
    x == b'\xff\xfe=\xd8%\xdd'
    len(x) == 6

See what happened? It chose a default of little-endian, and it stored
the byte order mark! But note what happens when I store *two* fire
emojis:

    x = "üî•üî•".encode("UTF-16")
    x == b'\xff\xfe=\xd8%\xdd=\xd8%\xdd'
    len(x) == 10

lol, see how this is *not* twice the length of a single fire emoji!!

**Red Heart**

Let's look at the red heart momentarily:

    x = "‚ù§Ô∏è".encode("UTF-16le")
    x == b"d'\x0f\xfe"
    len(x) == 4

And now in UTF-8:

    x = "‚ù§Ô∏è".encode("UTF-8")
    x == b'\xe2\x9d\xa4\xef\xb8\x8f'
    len(x) == 6

WTF? Well, let's see:

  b'\xe2\x9d\xa4'.decode("utf-8") == '‚ù§'

Ahh, you can see that this is two three-byte UTF-8 representations:
heart and "colorful." So the red heart is an example where UTF-16 wins
big!

Just to be clear, the length of a string counts the number of code
points, not the number of grapheme clusters:

    len("‚ù§Ô∏è") == 2

## C/C++

I believe that C++ will be happy to let you stick unicode characters
into strings:

```c
#include <stdio.h>

int main()
{
  printf("Hello world: üôÇ\n");

  return 0;
}
```

Presumably the file is written in UTF-8. The string literal is treated
as containing a series of bytes. Note that in UTF-8, ASCII bytes (0-127)
never occur when encoding non-ASCII codepoints into UTF-8. That is good,
and means UTF-8 is safe to just embed in a string in a C program.

This means that C programs can easily read and write UTF-8 strings.
However, the `strlen` function will obviously be wrong. It will continue
to return the number of *bytes* (not including the null terminator). But
obviously we expect that.

Let's work toward reading/writing individual unicode codepoints.

```c
#include <locale.h>
#include <stdio.h>
#include <wchar.h>

int main()
{
  // wchar_t is a 4-byte representation of a character. This can hold
  // all Unicode codepoints.
  wchar_t smileys[3] = {0x1F642, 0x1F642, 0};

  // You must call `setlocale` to inherit the locale from the terminal.
  // This will adopt UTF-8. That means that interpolation of wide
  // characters will encode them UTF-8 style.
  setlocale(LC_ALL, "");
  // Instead of simply writing %s, you write %ls which means you'll
  // provide a null-terminated wchar_t*. The code point will be encoded
  // appropriately.
  printf("here are two smileys: %ls\n", smileys);
  printf("here is a single smiley: %lc\n", smileys[0]);

  // You can also use wchar_t literals. Note that the character needs to
  // fit within a single wchar_t. The common red heart consists of two
  // code points, so you'll get a warning if you try L'‚ù§Ô∏è', and it won't
  // work like you expect.
  //
  // Basically, just like 'a' is just a standing for a char with a given
  // ASCII value, L'üôÇ' is just a wchar_t with a given value.
  wchar_t smiley = L'üôÇ';
  wchar_t checkbox = L'‚úÖ';

  printf("here is a single smiley again: %lc\n", smiley);
  printf("here is a checkbox: %lc\n", checkbox);

  return 0;
}
```

Now, let's try reading a unicode character and writing it back out:

```c
#include <locale.h>
#include <stdio.h>
#include <wchar.h>

int main()
{
  // We must set this before wide character reading or writing.
  setlocale(LC_ALL, "");

  // What is wint_t? First, recall that getc returns `int` rather than
  // `unsigned char`. Recall that it does this because it must be able
  // to return EOF, which is typically defined as -1.
  //
  // Shouldn't `getc` return `char`, in which case -1 is available? Not
  // really, since `getc` can be used for binary files, which don't
  // simply have bytes in the 0-127 range.
  //
  // With that recalled, we can see that getwc must be able to store all
  // wide characters. But an unsigned int can already store all the code
  // points we need, and more. Thus, WEOF is defined in GCC as
  // `(unsigned int) -1`.
  wint_t _character = getwc(stdin);

  // Unnecessary cast, but I do it for clarity. I could use a wint_t as
  // a wchar_t just as I could use an int from `getc` as a char.
  wchar_t character = (wchar_t)_character;

  printf("%lc\n", character);

  return 0;
}
```

Note that `wchar_t` is 32bits and uses UTF-32 internally on Linux. But
on Microsoft, it uses 16bits with a UTF-16 LE representation. I believe
that corresponds to Java and JavaScript.

`wchar_t` has been standardized in C/C++ for some time. The following
types are only in c++23, and not yet in ISO C I think.

You can specify what encoding you want with `char16_t` and `char32_t`.
You can specify literals as `u'üôÇ', U'üôÇ', or u"üôÇüôÇ", U"üôÇüôÇ"`. These
produce `char16_t`/`char32_t` or `char16_t*`/`char32_t*`. It's just a
more specific version of `wchar_t`.

Last, I can mention that `u8"üôÇ"` exists. If the default encoding is
UTF-8, then there is no difference from just using a regular string
literal. Note that `u8'üôÇ'` is bad, because the smiley is not just one
UTF-8 codepoint.

Everything I've said so far is not that useful. It's tough to operate
with unicode graphemes, because you need a library to help you
understand clusters. You have no idea how many characters wide a
character will print as.

## Sources

* https://blog.jonnew.com/posts/poo-dot-length-equals-two
