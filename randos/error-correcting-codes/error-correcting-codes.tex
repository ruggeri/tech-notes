\documentclass[11pt, oneside]{amsart}

\usepackage{geometry}
\geometry{letterpaper}

\usepackage{ned-common}
\usepackage{ned-abstract-algebra}

\begin{document}

\title{Error Correcting Codes}
\maketitle

\section{Parity}

To \emph{detect} errors, you can store \define{parity} bits per block.
These do not allow you to \emph{correct} errors. If this is a
transmission, then you can re-request a block that fails the parity
check.

With a single detection bit, you can detect a single bit-flip 100\% of
the time. The same holds for an odd number of bit flips. However, you
cannot detect \emph{any} even number of bit flips.

\subsection{Longitudinal and Transverse Redundancy Checks}

To get more sensitivity to errors, you can store \emph{multiple} parity
bits. One simple approach is to XOR the bytes of the file together. this
gives one final byte to append to the file. This is called a
\emph{longitudinal} redundancy check. The bytewise approach is sometimes
called BIP-8.

You may combine this with a \emph{transverse} redundancy check. This
calculates one bit of parity per byte. In the old days, IBM had \emph{9
track} recording media: eight data tracks and one parity track.

By combining both longitudinal and transverse redundancy checks, you can
\emph{correct} any one-bit flip. From the transverse check, you detect
what byte is in error. From the longitudinal check you know exactly what
\emph{bit} is in error.

\section{Cyclic Redundancy Check}

\subsection{Intro And Parity Example}

Here is another way to look at parity bits. The message bitstring of $n$
bits is actually a polynomial of $n-1$, with one-zero coefficients from
$\Zmod{2}$. The parity result is the \emph{remainder} of dividing the
message polynomial with an implementation specified \define{generator
polynomial}. The significance of the word \emph{generator} will be more
apparent soon.

Let the generator be $g(x) = x + 1$. Consider encoding various two-bit
messages 00, 01, 10, 11:

\begin{nedqn}
  0x + 0
\mapstocol
  0 (x + 1) + 0
\\
  0x + 1
\mapstocol
  0 (x + 1) + 1
\\
  1x + 0
\mapstocol
  1 (x + 1) + 1
\\
  1x + 1
\mapstocol
  1 (x + 1) + 0
\end{nedqn}

\noindent
This is identical to our standard parity calculation. You get the
remainder (i.e., the parity bit) by doing polynomial long-division and
using the remainder. Notice the \emph{third} scenario of 10 represented
as $x + 0$. Note how you will use 1 of $x + 1$ to eliminate the $x$
term, leaving $-1 \equiv +1$ as the remainder.

Note that by using a polynomial of degree 1 as your generator function,
you are left with a remainder that can be represented with 1 bit. To
create a 32 bit CRC \define{checksum}, you use a generator polynomial of
degree 33. There are many possible choices of generator polynomial -
each corresponds to a different implementation of CRC-32. There are a
number of factors to consider when choosing the generator.

\subsection{Calculation}

Because we are working $\Zmod{2}$, subtracting two polynomials is really
like doing an XOR of corresponding coefficients. For instance:

\begin{nedqn}
  &   & \parens{1x^5 + 0x^4 + 1x^3 + 1x^2 + 0x + 0}
\\
  & - & \parens{0x^5 + 1x^4 + 1x^3 + 0x^2 + 0x + 1}
\\
  & = & \parens{1x^5 + 1x^4 + 0x^3 + 1x^2 + 0x + 1}
\end{nedqn}

We can do polynomial long division to calculate a remainder by cycling
through bits of the message and doing an XOR by the generator polynomial
whenever we encounter a 1. Let's divide a polynomial by $x^3 + 1$.

\begin{IEEEeqnarray*}{+CCCCCCCCCCCC+}
         & 1x^5 & + & 0x^4 & + & 1x^3 & + & 1x^2 & + & 0x^1 & + & 0
\\
  \oplus & 1x^5 & + & 0x^4 & + & 0x^3 & + & 1x^2
\\
  \mapsto& 0x^5 & + & 0x^4 & + & 1x^3 & + & 0x^2 & + & 0x^1 & + & 0
\\
  \oplus &      &   &      &   & 1x^3 & + & 0x^2 & + & 0x^1 & + & 1
\\
  \mapsto& 0x^5 & + & 0x^4 & + & 0x^3 & + & 0x^2 & + & 0x^1 & + & 1
\end{IEEEeqnarray*}

\subsection{Error Polynomial}

A message polynomial $m(x)$ and a corrupted version $m'(x)$ are
indistinguishable modulo a generator polynomial $g(x)$ exactly when:

\begin{IEEEeqnarray*}{+rCll+}
  m(x) \equivcol m'(x) & \pmod{g(x)}
\\
  m(x) - m'(x) \equivcol 0 & \pmod{g(x)}
\end{IEEEeqnarray*}

\subsection{Detects All One Bit Errors}

We call $m(x) - m'(x)$ the \define{error polynomial}. We want to analyze
what kinds of $e(x)$ are equivalent to zero modulo $p(x)$. Consider
one-bit errors. In this case $e(x) = x^k$ for some $k$ (depending on the
position of the error in the message). When can $e(x)$ have $g(x)$ as a
factor?

I claim only when $g(x) = x^i$ for $i < k$. In particular, all one-bit
errors are detected if $g(x)$ has at least two terms. For consider any
$g(x)q(x)$. I claim this must always have at least two non-zero terms:
the product of the leading terms in $g(x), q(x)$, and the product of the
trailing terms in $g(x), q(x)$. These are two distinct terms, and thus
$x^k$ can never be a multiple of $g(x)$.

\subsection{Detects All Two Bit Errors Within A Window}

Consider two flipped bit errors. Then $e(x) = x^{k_1} + x^{k_2} =
x^{k_2} \parens{x^{k_1 - k_2} + 1}$. We've already proven that if $g(x)$
is chosen to have at least two terms, it cannot divide $x^{k_2}$. Let's
presume that $x^{k_2}$ and $g(x)$ share no common factors. That is
guaranteed if the constant term of $g(x)$ is 1. Then, if $g(x)$ is to
divide $e(x)$, it must divide $x^{k_1 - k_2} + 1$. Which is equivalent
to saying:

\begin{IEEEeqnarray*}{+rCll}
  x^{k_1 - k_2} + 1 \equivcol 0 \pmod{g(x)}
\\
  x^{k_1 - k_2} \equivcol 1 \pmod{g(x)}
\end{IEEEeqnarray*}

This basically is asking: what is the \define{order} of the polynomial
$x$ in the quotient field of polynomials over $\Zmod{2}$ modulo $g(x)$?
The greater the order of $x$, the greater the distance $k_1 - k_2$ over
which two bit flips can be detected. For good choices of $g(x)$ (called
\define{primitive polynomials}), $x$ will generate the polynomial field
entire multiplicative group. In this case, powers $x^i$ will cycle
through every non-zero polynomial of length $n$: all $2^n - 1$ of them!

Which means that $x^{k_1 - k_2} \equiv 1$ first when $k_1 - k_2 = 2^n -
1$.

Note: all primitive polynomials $g(x)$ must be \define{irreducible}.
Else there are two elements of the quotient ring $a(x), b(x)$ such that
$a(x)b(x) \equiv 0$. But then at least one of these must not be
generated by powers of $x$ else $x$ would eventually map back to zero!

\subsection{Detects All Odd Number Of Errors}

If one is willing to halve the window of two bit error detection, one
can detect all errors affecting an odd number of bits. Here we choose
$g(x)$ that is the product of (1) an irreducible polynomial $g_1(x)$ of
degree $n$ (not $n+1$ as usual) and (2) $g_2(x) = x + 1$.

The important thing is that $g(x)$ ought to have an \emph{even} number
of terms. A fact that I haven't proven are used: all irreducible
polynomials (excepting $x+1$) have an odd number of terms. If we believe
that, it's easy to see that $g(x) = g_1(x) (x+1)$ has an even number of
terms. This says that $g(x)$ is the XOR of two polynomials with an odd
number of terms. But if I flip an odd number of bits and then flip an
odd number again, I must have flipped an even in total (regardless
whether the two sets of flips overlap).

Now, if $g(x)$ has an even number of terms, no matter how many times we
XOR an error polynomial $e(x)$ with an odd number of terms, we'll still
be left with an odd number of remainder terms.

\subsection{Detects All Burst Errors Of Length At Most $n$}

A \define{burst error} is a subsequence of received bits where (1) the
first and last bits are in error, (2) no subsequence of $k$ bits in
between are correct. The $k$ is a parameter called the \define{guard
band} that can determine when you have \emph{one} or \emph{two} burst
errors. For instance, the end of one burst error comes at least $k$ bits
before the start of a next burst error. Note: burst errors can be of any
length, regardless the chosen guard band $k$.

If one chooses $k=1$, then burst errors are subsequences of received
bits that are \emph{all} in error. Anyway, we'll let $k = \infty$, so
that the entire corrupted range is treated as a single burst error.
We'll want to detect the burst error provided it is not too long.

This model of error is highly relevant to many communication and storage
media.

Consider an error polynomial for a burst. Then $e(x) = x^k e'(x)$. The
$x^k$ factor represents the position the burst error, while $e'(x)$ is a
polynomial representing the pattern of the error. We already know that
$g(x)$ divides $e(x)$ precisely if it divides $e'(x)$. But so long as
the degree of $e'(x)$ is less than the degree of $g(x)$, it cannot be
equivalent to zero.

Thus any burst error of at most $n$ symbols is detected!

\section{Transmission Error Control}

\subsection{Automatic Repeat Request}

ARQ means \define{automatic repeat request}. It is a protocol by which,
when the receiver detects a message block with an error, they re-request
that message block. Checksums are good for detecting errors. Here, there
is no need for the receiver to \emph{correct} errors because they can
simply re-request.

Automatic repeat request also means that the sender expects to receive
acknowledgments of blocks. If a block is unacknowledged for a timeout
duration, the sender will \emph{automatically} re-send the block. This
is typical of TCP packets.

\subsection{Stop-and-Wait ARQ}

There are a number of ARQ versions. The simplest is
\define{stop-and-wait ARQ}. Here, the sender sends a single dataframe,
waits for acknowledgment, and re-sends if a timeout expires. Only one
dataframe is ``in-flight'' at a time. Of course, one needs to be careful
if an ACK is lost. Then the receiver will be sent an \emph{second} copy
of the same dataframe, and they must be able to recognize that this is a
duplicate. Thus a \define{sequence number} is useful to embed in each
message. In the case of stop-and-wait, \emph{if we assume messages may
be lost but never re-ordered}, it is sufficient to use a \emph{one-bit}
sequence number.

\subsection{Go-Back-$n$ ARQ}

To utilize the channel better, the sender can push up to $n$ packets,
each with an incrementing sequence number. The receiver acknowledges
each, ignoring duplicated packets or any that skip a missing packet. The
sender, if they don't receive an ACK, will restart transmission from
that packet.

I believe the sequence number is incremented cyclically modulo $n$. If
the sender reset the sequence number to zero when it retransmits, the
receiver wouldn't know the last received ACK that got back to the
sender. Thus the receiver wouldn't know from what point the sender was
restarting.

This still only works if packets are delivered in order (but possibly
lost). It requires a buffer from the sender of $n$ packets.

I believe TCP uses a variant of Go-Back-$n$ ARQ.

\subsection{Selective Acknowledgment}

In this protocol, the receiver \emph{buffers} received packets, even if
an earlier packet has not been received. The receiver keeps track of the
earliest unreceived packet, and sends this number along with each
acknowledgment.

I won't go into more detail about this for now.

\subsection{Conclusion}

All of these protocols are specific versions of the more general
\define{sliding window protocol}. But I want to set this aside to move
on to \define{forward error correction}.

\section{Forward Error Correction}

\define{Forward error correction} is useful when retransmission of data
is either inconvenient or impossible. For instance: for data stored on
disk; the ``transmitted'' data is immediately forgotten. Forward error
correction is also useful for \define{broadcast transmission}, where the
sender is sending to many people, and there is no \define{backchannel}
by which receivers can send ACKs to the broadcaster (who presumably
couldn't process them anyway).

Low latency applications also may necessitate FEC because latency is
introduced if the sender needs to wait for an ACK timeout window. Or if
data cannot be buffered.

There are two kinds of error-correcting codes, generally speaking. One
set consists of the \define{convolutional codes}. These work on a
bit-by-bit basis. We'll focus mostly on \define{block codes}, which
operate per data block.

\section{Repetition Codes}

One of the simplest ECCs are the \define{repetition codes}. A basic
example is to repeat every bit sent three times. If only one of the
copies is corrupted, the receiver can correct the error. Repetition
codes are very \define{adaptive}, in that you can easily scale
up-or-down the error correcting ability based on how much noise is
encountered in the channel. You simply change how many times you repeat
the symbols.

Repetition codes are very inefficient.

\TODO{Why?}

\section{Hamming Notation and Concepts}

Hamming categorized codes by $(\mathit{numTotalBits},
\mathit{numDataBits})$. He assumes that all codewords will have the same
length, and that they will all carry $\mathit{numDataBits}$ of data and
$\parens{\mathit{numTotalBits} - \mathit{numDataBits}}$ bits of error
control information. He also assumes that the code was fixed: the
encoding of the next block does not depend on the data decoded in the
prior blocks.

Thus the $(3, 1)$-repetition code uses three bits to send one data bit.
ASCII is an $(8, 7)$ code. The \define{code rate} is the number of data
bits that are transmitted per encoding bit. In the case of the $(3,
1)$-repetition code this is $\frac{1}{3}$. Of course, it's always equal
to the second number divided by the first.

The \define{Hamming distance} between two (equal length) bit strings is
defined to be the number of bits that are different in the two strings.
An encoding is $k-1$-error-detecting if the Hamming distance between any
two valid \define{codewords} is at least $k$. That is: $k-1$ bit flip
errors in a codeword will always be detected.

The ASCII code has a Hamming distance of as little as two between
codewords. The $(3, 1)$-repetition code has a Hamming distance of
exactly three (there are only two valid codewords). Therefore one-bit
ASCII errors can always be detected, while up to two-bit $(3,
1)$-repetition errors can be detected.

We can also ask up to how many errors can be \emph{corrected}. This is
$\floor{\frac{k-1}{2}}$. Once the number of errors has taken us at least
half-way to another codeword, we lose the ability to correct reliably.
The corrupted codeword is now closer to the wrong codeword than the
right one.

Thus ASCII can correct no errors in a codeword, while $(3,
1)$-repetition can correct one bit.

Sometimes we notate codes as $(\textit{numTotalBits},
\textit{numDataBits}, \textit{codeDistance})$. Here
$\textit{codeDistance}$ means the minimum Hamming distance between two
codepoints. In studying Hamming codes, we'll look at codes where
$\textit{codeDistance} \geq 3$.

\section{Hamming Codes}

The question becomes: given the encoding constraints assumed by Hamming,
what codes most efficiently provide single-error-correction (SEC) and
double-error-detection (DED)? These are the \define{Hamming codes}. They
are the ones with the highest code-rate for a given
$\textit{numDataBits}$.

Note this requirement: if we change one data bit, we want to ensure that
this requires that we change \emph{at least} two parity bits.

\subsection{Description One}

Let there be $2^k - 1$ bits in the codewords. Number them $1, 2, 3,
\ldots, 2^k$ (one indexing!). Parity bits will live at each location
$2^i$, while data bits will live everywhere else. There will be $k$
parity bits. A parity bit at position $2^i$ covers all data bit
locations where bit $i$ is popped (zero indexing of bits!). The parity
bit stores the XOR of the data bit values.

Since there are $k$ parity bits, they can number all data bit locations
up to (and including) $2^k - 1$.

It should be clear that the only locations that would be covered by only
one parity bit are the parity bit locations themselves! Every data bit
is covered by at least two parity bits.

\subsection{Example: Hamming(7, 4)}

Here is the Hamming code with codeword length 7 and number of data bits
4:

\begin{nedqn}
  001 \mapstocol \textit{parityBit1} \text{ covers data bits } 011, 101, 111
\\
  010 \mapstocol \textit{parityBit2} \text{ covers data bits } 011, 110, 111
\\
  011 \mapstocol \textit{dataBit1}
\\
  100 \mapstocol \textit{parityBit3} \text{ covers data bits } 101, 110, 111
\\
  101 \mapstocol \textit{dataBit2}
\\
  110 \mapstocol \textit{dataBit3}
\\
  111 \mapstocol \textit{dataBit4}
\end{nedqn}

\subsection{Efficiency}

The Hamming codes have Hamming type $(2^k - 1, 2^k - k - 1)$. Thus they
have code rate:

\begin{nedqn}
  \frac{2^k - k - 1}{2^k - 1}
\eqcol
  1 - \frac{k - 1}{2^k - 1}
\end{nedqn}

Note that by increasing $k$ we can achieve high code rates (efficiency).
But at the same time we tolerate only 1 error per $2^k - 1$ bits, so our
reliability will go down. A good selection of $k$ will depend on the
reliability of the channel.

\subsection{Description Two}

This whole one-indexing situation feels weird. Here is a second
explanation of the code.

Let there be $\textit{numDataBits}$ data bits. We will assign these bits
names, which are unique bitstrings of length $\textit{numParityBits}$
with \define{Hamming weight} no less than two. That is: at least two
bits are popped in every name. A data bit's name will be used to
identify which parity bits cover it.

To name the data bits, we would typically use bitstrings with length
$\textit{numDataBits}$. But we need to skip $00\ldots0$ and we need to
skip every bitstring with exactly one bit popped (including
$00\ldots01$). The first data bit must be labeled $00\ldots011$.

If there are $\textit{numParityBits}$, then note that $numParityBits +
1$ bit strings are eliminated. This leaves $2^{\textit{numParityBits}} -
\parens{\textit{numParityBits} + 1}$ as eligible names. Thus we must
have:

\begin{nedqn}
  \textit{numDataBits}
\leqcol
  2^{\textit{numParityBits}} - \parens{\textit{numParityBits} + 1}
\end{nedqn}

We achieve equality when $\textit{numDataBits} = 2^k - {k - 1}$. In this
case, $\textit{numParityBits} = k$. Thus the we have $\textit{totalBits}
= 2^k - 1$.

\subsection{Single Error Correction}

Consider if a single data bit has been flipped in transmission. Then the
receiver will notice that some of the parity bits look wrong. The set of
parity bits that look wrong correspond to the ``name'' of the data
location with the flipped bit. We may thus correct that bit!

Next, consider if a \emph{parity} bit is flipped. A flipped parity bit
won't break any of the other parity bits, since they never cover this
position. The parity bit does not name any of the data bits. So we'll
see that exactly the parity bit is broken. We can fix this error (or
ignore it if we aren't worried about further errors accumulating).

\subsection{Achieving Double Error Detection}

The minimum Hamming distance between codewords is three. Thus if two
bits are flipped, we will be able to see that \emph{at least} one bit
was flipped, but if we do bit correction we may correct to the
\emph{wrong value}!

So you can extend the Hamming code with one more parity bit that covers
\emph{all} the values (both data and parity). The Hamming distance
between any two words is now four. Double errors can be detected!

To decode, first check the extended parity bit. If it is wrong, then you
can assume that two bit flips could not have occurred in the core
encoding. You may do standard decoding. BTW, if the core decoding
doesn't detect an error, then it was just the extra parity bit that got
flipped!

If the parity bit looks good, then either zero or two flips occurred.
Run the standard decoding. If the decoding detects no errors, then this
shows every is okay. If the core decoding detects one error (by design
it can never detect two!), then that means that either: (1) two of the
core bits were flipped, or (2) one of the core bits was flipped as well
as the extra parity bit. Since we can't eliminate the first case, we
must report failure.

Hamming codes are most useful where the error rate is low. It is widely
used for ECC memory. That kind of makes sense: Hamming codes can achieve
high code rates, but only if it is reasonable that there will be no more
than one or two errors per block.

\subsection{Hamming Bound and Perfect Codes}

Are Hamming codes as efficient as possible? We prove yes.

\begin{theorem}
  Consider the space $\Zmod{2}^k$. We wish to designate
  $\textit{numCodePoints}$ in the space, with the constraint that all
  codepoints are no less than Hamming distance $3$ from each other.
  Then:

  \begin{nedqn}
    \textit{numCodePoints}
  \leqcol
    \frac{2^k}{1 + k}
  \end{nedqn}

  This is called the \define{Hamming Bound}.
\end{theorem}

\begin{proof}
  Consider a codepoint to be a \emph{ball} in the space with Hamming
  radius 1. That is: it occupies a position $x$ (its center), plus every
  adjacent location in the space. Each ball occupies $1 + k$ spots in
  the space. One for its center, and $k$ for each adjacent location in
  the $k$ dimensions.

  But this exactly implies implies the bound we want.
\end{proof}

\begin{definition}
  A code which corrects one failure and obtains the Hamming Bound is
  called a \define{perfect} code.
\end{definition}

\begin{corollary}
  Any Hamming code is perfect.
\end{corollary}

\begin{proof}
  Let $k = \textit{numTotalBits} = 2^m - 1$. Then:

  \begin{nedqn}
    \textit{numCodePoints}
  \leqcol
    \frac{2^{2^m - 1}}{2^m}
  \\
    \logf{\textit{numCodePoints}}
  \leqcol
    2^m - 1 - m
  \end{nedqn}

  But we know that the Hamming code Hamming($2^m - 1$, $2^m - m - 1$)
  meets exactly this bound.
\end{proof}

\subsection{Motivation}

Block codes treat each chunk of message symbols equally. This would be
inefficient if message chunks contained mutual information. However, we
can assume that a compression algorithm has ensured that the mutual
information between any two non-overlapping sets of bits is zero. In
this case the method of encoding chunks of message bits makes sense.

A second presumption is that it handles bit corruption and not insertion
or deletion of bits. This probably makes sense in many contexts. If
something is being transmitted, high/low amplitudes may be held for fix
durations. So long as transmitting/receiving equipment uses synchronized
clock rates, then no bits should be inserted/added.

It is unnatural that Hamming codes cannot handle \emph{burst errors},
which are expected to often be common.

% ## Reed-Solomon

% Reed-Solomon codes are widely used, they are error-correcting, and can
% also detect missing data. Codes like this are called *erasure codes*.

% Reed-Solomon, given `t` check symbols, can:

% * Detect `t` erroneous symbols.
% * Correct `FLOOR(t/2)` erroneous symbols.
% * It can correct up to `t` erasures.

% With larger symbols, you can handle burst errors well, since this
% corrupts a single (or a few) symbols. This makes it useful for CDs,
% DVDs, where burst errors are common (misaligned laser).

% When Reed-Solomon codes were created, an *efficient* decoding scheme
% did not exist. That came almost a decade later, which allowed for
% their practical use.

% The math looks pretty hard on this, and I don't really care that
% much. Let's just say **Mission Accomplished**.

% # TODO

% * Rolling Hash
% * md5sum
%
% Reed-Solomon
%
% https://en.wikipedia.org/wiki/Block_code
% https://en.wikipedia.org/wiki/Linear_code
% https://en.wikipedia.org/wiki/Noisy-channel_coding_theorem
% https://en.wikipedia.org/wiki/Parchive
% https://en.wikipedia.org/wiki/RAID

\end{document}
