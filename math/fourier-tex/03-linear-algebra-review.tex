\section{Linear Algebra Review}

Breaking something apart into its constituents is something we do in
\define{linear algebra}. The space of waveforms is a vector space. We
want to decompose a vector $\vv$ into a linear combination of basis
vectors. That's linear algebra.

The function that does decomposing is called a \define{inner product}.
Let's explore what kind of inner product we'll need.

\subsection{Defining Length, Angle, and Projection}

Consider a vector space and a basis for the space $\mathcal{B}$. We will
declare the basis vectors to have ``unit length.'' We will declare that
any rotation between unit vectors is also a unit vector. Recall that a
``rotation'' between two vectors can simply be defined algebraically:

\begin{nedqn}
  \cos\theta \vu + \sin\theta \vv
\end{nedqn}

Since every vector $\vv$ is a scalar multiple of a unit vector (easy to
prove), we can extend our notion of ``length'' to non-unit vectors. Let
$\norm{\vv} = \alpha$ if $\vv = \alpha \vw$ where $\vw$ is a unit-vector
(rotation of basis vectors).

We'll declare the basis vectors to be ``orthogonal.'' We can go further
and define the ``angle'' between two arbitrary unit vectors $\vu, \vv$.
We can start defining this notion by saying the angle between $\ve_i$
and $\vv$. I say that there exists some unique pair $\vw$ and $\theta$
such that: (1) $\vw$ is in the subspace spanned by $\setof{\ve_j | j \ne
i}$, and (2):

\begin{nedqn}
  \vv
\eqcol
  \cos\theta \ve_i + \sin\theta \vw
\end{nedqn}

\noindent
I call $\theta$ the angle between $\ve_i$ and $\vv$. It's the angle
needed to rotate $\ve_i$ in the direction of some (orthogonal) $\vw$
such that the result is $\vv$.

Why do we know that $\vw$ exists? First, we know that $\vv$ lies in
\emph{some} plane defined by $\ve_i$ and some $\vw$, since the basis
spans the vector space. We might as well assume $\vw$ is unit-length.
But then all unit-length vectors in this plane are rotations of $\vu$
toward $\vw$. This (almost) uniquely specifies $\theta$, up to two
choices: $\theta$ vs $2\pi - \theta$.

We can further extend the notion of angle-between to any two unit
vectors by saying that angle-between should be rotation-invariant.

Once we have this notion of angle-between nailed down, we can
unambiguously define the \define{inner product} $\innerprod{\vu}{\vv} =
\cos\theta$, where $\theta$ is the angle in between two unit vectors. We
can then further extend this to vectors of arbitrary length by defining
$\innerprod{\vu}{\vv} = \norm{\vu} \norm{\vv} \cos\theta$. We call
$\frac{\innerprod{\vu}{\vv}}{\norm{\vv}} \vv$ the \define{projection} of
$\vu$ onto $\vv$.

Notice that since we are using $\cos\theta$, it doesn't matter the sign
of $\theta$. We could iron out the sign wrinkles of our
``angle-between'' concept, but we don't need to.

So, if we have a basis for a vector space, this suggests a notion of
``angle-between,'' ``length,'' and even ``projection.'' Alternatively,
given a pre-existing notion of projection defined by
$\innerprod{\cdot}{\cdot}$, any orthonormal basis $\mathcal{B}$ would be
equally ``compatible'' with this inner-product. That is: the standard
construction of angle/length/projection from $\mathcal{B}$ would exactly
correspond to the inner product $\innerprod{\cdot}{\cdot}$ we started
with.

\subsection{Projection Properly Projects/Decomposes}

We have called $\innerprod{\vv}{\vu}$ the ``projection'' of $\vv$ onto
$\vu$. This terminology is compatible with our usual geometric
understanding of ``projection.'' If, in $n$-dimensional Euclidean space,
we rotate $\vu$ by $\theta$ degrees and get $\vv$, then it's true that
the length of the projection of $\vv$ onto $\vu$ is:

\begin{nedqn}
  \norm{\vv} \cos\theta
\eqcol
  \frac{\innerprod{\vv}{\vu}}{\norm{\vu}}
\end{nedqn}

Also, the inner-product gives us the way to decompose a vector $\vu$ into
a linear combination of $\ve_i$ in an orthonormal basis $\mathcal{B}$:

\begin{nedqn}
  \innerprod{\vv}{\ve_i}
\eqcol
  \innerprod{\sum_{j=1}^n \alpha_j \ve_j}{\ve_i}
\\
\eqcol
  \sum_{j=1}^n \alpha_j \innerprod{\ve_j}{\ve_i}
  \nedcomment{by linearity}
\\
\eqcol
  \alpha_i
  \nedcomment{other $\alpha_j$ cancel by orthonormality}
\end{nedqn}

\noindent
This proof relied on (1) the basis is orthonormal, (2) the inner product
is \define{linear} in the first argument. We will show later why the
inner-product must be linear in the first argument.

But we don't need to adopt this proof that comes from linearity. We
could instead rely on our earlier proof that (1) $\vv$ is always the
result of rotating $\ve_i$ by some $\theta$ in the direction of some
$\vw$ that lives in the subspace spanned by the other basis vectors, and
(2) $\innerprod{\vv}{\ve_i} = \cos\theta$. In that case, it's clear that
$\alpha_i = \cos\theta$. To find the other $\setof{\alpha_j}$ to
complete the decomposition into a linear combination, we decompose
$\sin\theta \vw$ in the subspace orthogonal to $\ve_i$. This proves that
$\innerprod{\cdot}{\cdot}$ properly decomposes unit-vectors; the proof
extends trivially to scaling.

To tie things up: consider an orthonormal basis $\mathcal{B}$. Then:

\begin{nedqn}
  \vv
\eqcol
  \sum_{i=1}^n \innerprod{\vv}{\vb_i} \vb_i
\end{nedqn}

\subsection{Inner products are always dot products}

Let's finally show that an inner-product needs to act like a \define{dot
product}.

One way to show this is simply to appeal to $n$-dimensional Euclidean
space. Any $n$-dimensional vector space $V$ has the same notions of
angle and length as an $n$-dimensional Euclidean space (where the
coordinate vectors correspond to a set of orthonormal basis vectors for
$V$). If you already know that the dot-product does the decomposing in
an $n$-dimensional Euclidean space, then we are done.

But let's not make this appeal. We'll show that an inner-product must
always be a dot product. To do this, it suffices to show: (1) linearity
in first argument, and (2) symmetry of the inner product. Then we have:

\begin{nedqn}
  \innerprod{\vu}{\vv}
\eqcol
  \innerprod{
    \sum \alpha_i \ve_i
  }{
    \sum \beta_j \ve_j
  }
\\
\eqcol
  \sum
  \alpha_i
  \innerprod{\ve_i}{\sum \beta_j \ve_j}
  \nedcomment{linearity in first argument}
\\
\eqcol
  \sum
  \alpha_i
  \innerprod{\sum \beta_j \ve_j}{\ve_i}
  \nedcomment{symmetry}
\\
\eqcol
  \sum_{i=1}^n
  \sum_{j=1}^n
  \alpha_i
  \beta_j
  \innerprod{\ve_j}{\ve_i}
  \nedcomment{linearity}
\\
\eqcol
  \sum \alpha_i \beta_i
  \nedcomment{orthonormality}
\end{nedqn}

\noindent
That is: an inner-product can always be calculated by doing a dot
product where the vectors are decomposed into tuple-representation with
respect to a common orthonormal basis. Or in short: inner products are
always dot products.

We can start by proving symmetry. Consider units $\vu, \vv$. Then
$\innerprod{\vu}{\vv} = \cos\theta$ by definition. But note that the
angle between $\vu, \vv$ is irrespective of the ordering of $\vu, \vv$.
Or perhaps it changes from $\theta$ to $2\pi - \theta$. But who cares:
$\cos\theta$ stays the same. So the inner product must be symmetric.

That means we need to show linearity. We can consider $\innerprod{\vu +
\vv}{\vw}$. We could geometrically prove this by adding $\vu$ and $\vv$
head-to-tail. We would see that the length of the projection of the sum
is equal to the sum of the lengths of the projections.

We'll do exactly that, with just a little more algebra/rigor. The
projection operation doesn't care about the components of $\vu, \vv$
orthogonal to $\vw$. So we might as well throw them out and just
consider $\vu' = \norm{\vu} \cos\theta_u \vw$ and $\vv' = \norm{\vv}
\cos\theta_v \vw$. But then it is clear that the projection of $\vu' +
\vv'$ onto $\vw$ is the sum of their projections.

This establishes linearity in the first argument. Putting these two
facts (linearity, symmetry) together, we have that every inner-product
is simply a dot-product with respect to any orthonormal basis.
