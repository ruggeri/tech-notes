\documentclass[11pt, oneside]{amsart}

\usepackage{geometry}
\geometry{letterpaper}

\usepackage{ruggeri_common}
\usepackage{ruggeri_linear_algebra}

\begin{document}

\title{Every Real Symmetric Matrix Is Diagonalizable}
\maketitle

\include{01-abstract.tex}
\include{02-proposition.tex}
\include{03-original-attempt.tex}
\include{04-contrast-with-gaussian-elimination.tex}
\include{05-exact-algorithm-for-eigenvectors}
\include{06-the-corrected-proof-optimization-goal.tex}
\include{07-how-rotations-change-a.tex}
\include{08-first-order-conditions.tex}

\section{
  Calculation Of
  $
    \fptheta \left[
      \left( \mtxA'_{i, j}(\theta) \right)^2
    \right]
  $
}

Let's do this. First we simply apply the chain rule.

\begin{nedqn}
  \fptheta \left[
    \left( \mtxA'_{i, j}(\theta) \right)^2
  \right]
& = &
  2
  \mtxA'_{i, j}(\theta)
  \fptheta \left[ \mtxA'_{i, j}(\theta) \right]
  \nednumber\label{partial:i:j:error}%
\end{nedqn}

Next, let's recall our formula for $\mtxA'_{i, j}$.

\begin{nedqn}
  \mtxA'_{i, j}(\theta)
& = &
  {
    (\sin\theta\cos\theta)
    (\mtxA_{j, j} - \mtxA_{i, i})
    +
    (\cos^2\theta - \sin^2\theta)
    \mtxA_{i, j}
  }
  \nedcommenthard{i.e., formula \eqref{change:i:j}}%
\end{nedqn}

The chain rule wants us to calculate the derivative for this:

\begin{nedqn}
  \fptheta \left[
    \mtxA'_{i, j}(\theta)
  \right]
& = &
  \fptheta \Bigl[
    (\sin\theta \cos\theta)
    (\mtxA_{j, j} - \mtxA_{i, i})
    +
    (\cos^2\theta - \sin^2\theta)
    \mtxA_{i, j}
  \Bigr]
  \\
& = &
  (\cos^2\theta - \sin^2\theta)
  (\mtxA_{j, j} - \mtxA_{i, i})
  \\&&
  \phantom{(}
  +
  (-2\cos\theta\sin\theta - 2\sin\theta\cos\theta)
  \mtxA_{i, j}
  \\
& = &
  (\cos^2\theta - \sin^2\theta)
  (\mtxA_{j, j} - \mtxA_{i, i})
  -
  4
  \cos\theta \sin\theta
  \mtxA_{i, j}
  \nednumber\label{partial:i:j}%
\end{nedqn}

Next, we must evaluate $\mtxA'_{i, j}(0)$ and $\fptheta \mtxA'_{i,
j}(0)$.

\begin{nedqn}
  \mtxA'_{i, j}(0)
& = &
  (\sin0 \cos0)
  (\mtxA_{j, j} - \mtxA_{i, i})
  +
  (\cos^2 0 - \sin^2 0) \mtxA_{i, j}
  \\
& = &
  \mtxA_{i, j}
  \nednumber%
  \label{aprime:i:j:at:zero}%
  %
\shortintertext{and}
  %
  \fptheta \left[
    \mtxA'_{i, j}(0)
  \right]
& = &
  (\cos^2 0 - \sin^2 0)
  (\mtxA_{j, j} - \mtxA_{i, i})
  -
  4
  \left( \cos 0 \sin 0 \right) \mtxA_{i, j}
  \\
& = &
  (\mtxA_{j, j} - \mtxA_{i, i})
  \nednumber%
  \label{partial:i:j:at:zero}%
\end{nedqn}

We may now evaluate the partial for $(\mtxA'_{i, j})^2$ at $\theta = 0$
by substitution:

\begin{nedqn}
  \fptheta \left[
    \left( \mtxA'_{i, j}(0) \right)^2
  \right]
& = &
  2
  \mtxA'_{i, j}(0)
  \fptheta \left[ \mtxA'_{i, j}(0) \right]
  \\
& = &
  2
  \mtxA_{i, j}
  (\mtxA_{j, j} - \mtxA_{i, i})
  \nednumspace%
  \nednumber%
\end{nedqn}

Of course, we may use this to evaluate $\fptheta f(0)$, as has been
our goal:

\begin{nedqn}
  \fptheta \left[ f(\theta) \right]
  \!
  (0)
& = &
  \fptheta \left[
    \left( \mtxA'_{i, j}(0) \right)^2
  \right]
  +
  \fptheta \left[
    \left( \mtxA'_{j, i}(0) \right)^2
  \right]
  \\
& = &
  2 \fptheta \left[
    \left( \mtxA'_{i, j}(0) \right)^2
  \right]
  \\
& = &
  4
  \mtxA_{i, j}
  (\mtxA_{j, j} - \mtxA_{i, i})
  \nednumber%
\end{nedqn}

Let's sum up. We used these facts:

\begin{itemize}
  \item there is no change to entries outside the $i$th and $j$th
  rows/columns,

  \item the effect on $f$ of changes at positions $(k, i)$ cancel with
  the effect of changes at $(k, j)$ (for $k \not\in \setof{i, j}$),

  \item likewise for positions $(i, k)$ and $(j, k)$,

  \item changes at $(i, i)$ and $(j, j)$ never count toward the error
  because they are on-diagonal,

  \item that leaves only positions to consider $(i, j)$ and $(j, i)$.
\end{itemize}

We may finally say that if $\mtxA$ minimizes $f$, then:

\begin{IEEEeqnarray*}{rrCl}
  &
  \fptheta \left[ f(0) \right]
  =
  0
\\
  \Rightarrow{} &
  4 \mtxA_{i, j} (\mtxA_{j, j} - \mtxA_{i, i})
  & = &
  0
\\
  \Rightarrow{} &
  \mtxA_{i, j} (\mtxA_{j, j} - \mtxA_{i, i})
  & = &
  0
  \nednumber%
\end{IEEEeqnarray*}

Which of course implies:

\begin{nedqn}
  \mtxA_{i, j} = 0
& \quad\text{or}\quad &
  (\mtxA_{j, j} - \mtxA_{i, i}) = 0
\end{nedqn}

The first condition is exactly what we want: it would impose a
requirement that the matrix $\mtxA$ have zero elements off the diagonal
in order to minimize $f$. That is: $f$ would achieve a minimum only for
a diagonal matrix. Since we know $f$ indeed achieves a minimum, it must
be that $\mtxA$ is diagonalizable.

But the second condition is an ``escape hatch.'' It says that the entry
off the diagonal needn't be zero so long as $\mtxA_{j, j} = \mtxA_{i,
i}$. Uh-oh?

% TODO: Up to here in my review.

\section{An Example}

Are we fucked? Presumably not. Let's just look at one matrix to make
ourselves feel a little better:

\begin{equation*}
  \begin{bmatrix}
    1 & 2 \\
    2 & 1
  \end{bmatrix}
\end{equation*}

What happens when we try to rotate it?

\begin{equation*}
  \begin{split}
      &\begin{bmatrix}
        \cos\theta & -\sin\theta \\
        \sin\theta & \cos\theta
      \end{bmatrix}\tran
      \begin{bmatrix}
        1 & 2 \\
        2 & 1
      \end{bmatrix}
      \begin{bmatrix}
        \cos\theta & -\sin\theta \\
        \sin\theta & \cos\theta
      \end{bmatrix}
    \\
    ={}&
      \begin{bmatrix}
        \cos\theta  & \sin\theta \\
        -\sin\theta & \cos\theta
      \end{bmatrix}
      \begin{bmatrix}
        1\cos\theta + 2\sin\theta & -1\sin\theta + 2\cos\theta \\
        2\cos\theta + 1\sin\theta & -2\sin\theta + 1\cos\theta
      \end{bmatrix}
    \\
    ={}&
      \begin{bmatrix}
        1 \cos^2\theta + (2 + 2) \cos\theta \sin\theta + 1 \sin^2\theta
        &
        2 \cos^2\theta + (-1 + 1) \cos\theta \sin\theta - 2\sin^2\theta
        \\
        2 \cos^2\theta + (-1 + 1) \cos\theta \sin\theta - 2\sin^2\theta
        &
        1\cos^2\theta + (-2 - 2) \cos\theta\sin\theta + 1\sin^2\theta
      \end{bmatrix}
    \\
    ={}&
      \begin{bmatrix}
        1 + 4\cos\theta\sin\theta
        &
        2 \left( \cos^2\theta - \sin^2\theta \right)
        \\
        2 \left( \cos^2\theta - \sin^2\theta \right)
        &
        1 - 4\cos\theta\sin\theta
      \end{bmatrix}
    \end{split}
\end{equation*}

Okay let's calculate $\fptheta f$ for this matrix!

\begin{equation*}
  \begin{split}
      \fptheta \left[f(\theta)\right]
    ={}&
      \fptheta \left[
        \left( \mtxA'_{2, 1}(\theta) \right)^2
        + \left( \mtxA'_{1, 2}(\theta) \right)^2
      \right]
    \\
    ={}&
      4
      \mtxA'_{2, 1}(\theta)
      \fptheta \left[ \mtxA'_{2, 1}(\theta) \right] \\
    \\
      \mtxA'_{2, 1}(\theta)
    ={}&
      2 \left( \cos^2\theta - \sin^2\theta \right) \\
      \mtxA'_{2, 1}(0)
    ={}&
        2 \left( \cos^2 0 - \sin^2 0 \right) \\
    ={}&
      2 \\
    \\
      \fptheta \left[ \mtxA'_{2, 1}(\theta) \right]
    ={}&
      \fptheta \left[
        2 \left( \cos^2\theta - \sin^2\theta \right)
      \right] \\
    ={}&
      2 \big( -2\cos\theta \sin\theta - 2\sin\theta\cos\theta \big) \\
    ={}&
      -8 \cos\theta \sin\theta \\
      \fptheta \left[ \mtxA'_{2, 1}(0) \right]
    ={}&
      -8 \cos 0 \sin 0 \\
    ={}&
      0 \\
    \\
      \fptheta \left[f(0)\right]
    ={}&
      4
      \mtxA'_{2, 1}(0)
      \fptheta \left[ \mtxA'_{2, 1}(0) \right] \\
    ={}&
      4 \cdot 2 \cdot 0 \\
    ={}& 0
  \end{split}
\end{equation*}

All our calculations check out with what we've done before. And we know
that, because this is a 2-by-2 real symmetric matrix, it must be
diagonalizable. And still the first partial is zero\dots

Well, the solution is clear: what is the story with the \textit{second}
partial?

\begin{equation*}
  \begin{split}
      \fpthetax \left[f(\theta)\right]
    ={}&
      \fptheta \Big[ \fptheta \left[f(\theta)\right] \Big] \\
    ={}&
      \fptheta \Big[
        4
        \mtxA'_{2, 1}(\theta)
        \fptheta \left[ \mtxA'_{2, 1}(\theta) \right]
      \Big] \\
    ={}&
      4 \Big[
        \fptheta \left[ \mtxA'_{2, 1}(\theta) \right]
        \fptheta \left[ \mtxA'_{2, 1}(\theta) \right]
        +
        \mtxA'_{2, 1}(\theta)
        \fpthetax \left[ \mtxA'_{2, 1}(\theta) \right]
      \Big] \\
    \\
      \fpthetax \left[ \mtxA'_{2, 1}(\theta) \right]
    &=
      \fptheta \Big[ -8 \cos\theta \sin\theta \Big] \\
    &= -8 \left( \cos^2\theta - \sin^2\theta \right) \\
      \fpthetax \left[ \mtxA'_{2, 1}(0) \right]
    &= -8 \left( \cos^2 0 - \sin^2 0 \right) \\
    &= -8 \\
    \\
      \fpthetax \left[f(0)\right]
    ={}&
      4 \Big[
        \fptheta \left[ \mtxA'_{2, 1}(0) \right]
        \fptheta \left[ \mtxA'_{2, 1}(0) \right]
        +
        \mtxA'_{2, 1}(0)
        \fpthetax \left[ \mtxA'_{2, 1}(0) \right]
      \Big] \\
    ={}&
      4 \Big[
        \left(0 \cdot 0\right)
        +
        \left(2 \cdot -8\right)
      \Big] \\
    ={}& -64
  \end{split}
\end{equation*}

Wonderful. In conclusion, we see that indeed the second partial is
negative, which means that we are at a local maximum.

\section{General Second Order Conditions}

Having verified that things work out in an example, let's return to the
general setting. We want to show that the second partial must always be
negative if $A_{j, j} - A_{i, i} = 0$ but $A_{i, j} \ne 0$.

\begin{equation*}
  \begin{split}
      \fpthetax \left[ \left( \mtxA'_{i, j}(\theta) \right)^2 \right]
    ={}&
      \fptheta \Big[
        \fptheta \left[ \left( \mtxA'_{i, j}(\theta) \right)^2 \right]
      \Big] \\
    ={}&
      \fptheta \Big[
        2
        \mtxA'_{i, j}(\theta)
        \fptheta \left[ \mtxA'_{i, j}(\theta) \right]
      \Big] \\
    ={}&
      2 \Big[
        \left( \fptheta \left[ \mtxA'_{i, j}(\theta) \right] \right)^2
        +
        \mtxA'_{i, j}(\theta)
        \fpthetax \left[ \mtxA'_{i, j}(\theta) \right]
      \Big] \\
    ={}&
      2 \Big[
        \mtxA'_{i, j}(\theta)
        \fpthetax \left[ \mtxA'_{i, j}(\theta) \right]
      \Big]
  \end{split}
\end{equation*}

The last step is justified because we wouldn't bother with the second
order test unless $\fptheta \left[ \mtxA'_{i, j}(\theta) \right] = 0$.
We've already calculated $\mtxA'_{i, j}(\theta)$ previously, so let's
focus on $\fpthetax \left[ \mtxA'_{i, j}(\theta) \right]$.

\begin{equation*}
  \begin{split}
      \fpthetax \left[ \mtxA'_{i, j}(\theta) \right]
    &=
      \fptheta \Big[ \fptheta \left[ \mtxA'_{i, j}(\theta) \right] \Big] \\
    &=
      \fptheta \Big[
        (\cos^2\theta - \sin^2\theta) (\mtxA_{j, j} - \mtxA_{i, i})
        - 4 \cos\theta\sin\theta \mtxA_{i, j}
      \Big] \\
    &=
      \fptheta \Big[
        -4\cos\theta\sin\theta \mtxA_{i, j}
      \Big]
  \end{split}
\end{equation*}

Notice that I've used our presumption that $\mtxA_{j, j} = \mtxA_{i,
i}$, since otherwise the first-order condition would have already
ensured that the off-diagonal $\mtxA_{i, j}$ were zero). We continue:

\begin{equation*}
  \begin{split}
      \fpthetax \left[ \mtxA'_{i, j}(\theta) \right]
    &=
      \fptheta \Big[ -4\cos\theta\sin\theta \mtxA_{i, j} \Big] \\
    &=
      -4 \left( \cos^2\theta - \sin^2\theta \right) \mtxA_{i, j}
  \end{split}
\end{equation*}

And we may now plug this in:

\begin{equation*}
  \begin{split}
      \fpthetax \left[ \left( \mtxA'_{i, j}(\theta) \right)^2 \right]
    ={}&
      2 \Big[
        \mtxA'_{i, j}(\theta)
        \fpthetax \left[ \mtxA'_{i, j}(\theta) \right]
      \Big] \\
    ={}&
      2 \mtxA'_{i, j}(\theta) \Big(
        -4 \left( \cos^2\theta - \sin^2\theta \right) \mtxA_{i, j}
      \Big) \\
    ={}&
      -8 \mtxA'_{i, j}(\theta)
      \left( \cos^2\theta - \sin^2\theta \right) \mtxA_{i, j} \\
  \end{split}
\end{equation*}

There is nothing left to do but evaluate at $\theta = 0$:

\begin{equation*}
  \begin{split}
      \fpthetax \left[ \left( \mtxA'_{i, j}(0) \right)^2 \right]
    ={}&
      -8 \mtxA'_{i, j}(0)
      \left( \cos^2 0 - \sin^2 0 \right) \mtxA_{i, j} \\
    ={}&
      -8 \mtxA'_{i, j} \mtxA_{i, j} \\
    ={}&
      -8 \left( \mtxA'_{i, j} \right)^2 \\
  \end{split}
\end{equation*}

And there you have it. This second derivative must always be negative,
since it is $-8$ (always negative) times $\left( \mtxA_{i, j} \right)^2$
(always positive). (Note that we assumed $\mtxA_{i, j} \ne 0$, otherwise
there was no reason to go down this road.)

As a quick check from our previous example: if you have $\mtxA_{i, j} =
2$, then $\fpthetax \left[ \left( \mtxA'_{i, j}(0) \right)^2 \right] =
-32$. If you then double this (to account for both $\mtxA'_{i, j}$ and
$\mtxA'_{j, i}$), you get back $-64$ as I calculated previously.

\section{Conclusion}

Let's recap what we have done.

\begin{enumerate}
  \item We proved (elsewhere, previously) that rotation of basis keeps
  symmetric matrices symmetric.

  \item We then defined an ``error'' function $f$ to minimize. The error
  is minimal (zero) precisely when all off-diagonal entries are zeroed.

  \item Since the space of rotations of $\mtxA$ is compact, and because
  $f$ is continuous, we know it achieves a minimum in this space.

  \item We were unsure whether the minimum of $f$ on the space of
  rotations is actually zero.

  \item We use the first derivative test to discover that for any
  optimal $\mtxA$, either $\mtxA_{i, j} = \mtxA_{j, i} = 0$ (as desired)
  OR $\mtxA_{i, i} = \mtxA_{j, j}$ (leaving open the possibility that
  $\mtxA_{i, j} \ne 0$).

  \item We then examined what the second derivative would be at a
  first-order critical point, in the undesired case that $\mtxA_{i, j}
  \ne 0$, and thus necessarily $\mtxA_{i, i} = \mtxA_{j, j}$.

  \item The second derivative test showed us this must be a local
  \textit{maximum}. Thus we can preclude this scenario at a global
  minimum.

  \item We have foreclosed any escape. A global minimum must have
  $\mtxA_{i, j} = \mtxA_{j, i} = 0$, else it would be possible to
  improve.
\end{enumerate}

Does this proof have any advantage over the much more succinct proof JRN
gave me?

This proof accords with an intuition that you can always iteratively
keep improving your matrix by doing little rotations. It shows that you
can never get stuck. Indeed, you can use gradient descent if you want.
So this proof is much more clearly tied to a notion of
\textit{procedure}, which is certainly related to intuition.

Second, the proof says that, if you believe things work in two
dimensions, you can just keep picking pairs of basis vectors to fix, and
do your best. Remember, you might ``disrupt'' previous work, but to the
extent you disrupt previous work, you're helping other work. That's what
we know from the cancelation of the $\mtxA_{k, i}, \mtxA_{k, j}$ terms.

Probably that would have been a simpler proof right there. You have this
iterative process now, and it must converge to an answer, so the answer
must exist\dots

One goal was to limit the magical use of symmetry. Here it is still used
somewhat: you use symmetry when you presume that rotation of basis
preserves symmetry. Likewise, you use symmetry when you assume that
fixing things in one subspace won't have an overall negative impact.

TODO: Come back and think more about whether this proof really does feel
less mysterious?

\end{document}
