\documentclass[11pt, oneside]{amsart}

\usepackage{geometry}
\geometry{letterpaper}

\usepackage{ruggeri_common}
\usepackage{ruggeri_linear_algebra}

\begin{document}

\title{Every Real Symmetric Matrix Is Diagonalizable}
\maketitle

\input{01-abstract}
\input{02-proposition}
\input{03-original-attempt}
\input{04-contrast-with-gaussian-elimination}
\input{05-exact-algorithm-for-eigenvectors}
\input{06-the-corrected-proof-optimization-goal}
\input{07-how-rotations-change-a}
\input{08-first-order-conditions}
\input{09-calculation-of-derivative-for-cross-terms}
\input{10-summary-thus-far}
\input{11-second-order-conditions}

\section{An Example}

Are we fucked? Presumably not. Let's just look at one matrix to make
ourselves feel a little better:

\begin{equation*}
  \begin{bmatrix}
    1 & 2 \\
    2 & 1
  \end{bmatrix}
\end{equation*}

What happens when we try to rotate it?

\begin{equation*}
  \begin{split}
      &\begin{bmatrix}
        \cos\theta & -\sin\theta \\
        \sin\theta & \cos\theta
      \end{bmatrix}\tran
      \begin{bmatrix}
        1 & 2 \\
        2 & 1
      \end{bmatrix}
      \begin{bmatrix}
        \cos\theta & -\sin\theta \\
        \sin\theta & \cos\theta
      \end{bmatrix}
    \\
    ={}&
      \begin{bmatrix}
        \cos\theta  & \sin\theta \\
        -\sin\theta & \cos\theta
      \end{bmatrix}
      \begin{bmatrix}
        1\cos\theta + 2\sin\theta & -1\sin\theta + 2\cos\theta \\
        2\cos\theta + 1\sin\theta & -2\sin\theta + 1\cos\theta
      \end{bmatrix}
    \\
    ={}&
      \begin{bmatrix}
        1 \cos^2\theta + (2 + 2) \cos\theta \sin\theta + 1 \sin^2\theta
        &
        2 \cos^2\theta + (-1 + 1) \cos\theta \sin\theta - 2\sin^2\theta
        \\
        2 \cos^2\theta + (-1 + 1) \cos\theta \sin\theta - 2\sin^2\theta
        &
        1\cos^2\theta + (-2 - 2) \cos\theta\sin\theta + 1\sin^2\theta
      \end{bmatrix}
    \\
    ={}&
      \begin{bmatrix}
        1 + 4\cos\theta\sin\theta
        &
        2 \left( \cos^2\theta - \sin^2\theta \right)
        \\
        2 \left( \cos^2\theta - \sin^2\theta \right)
        &
        1 - 4\cos\theta\sin\theta
      \end{bmatrix}
    \end{split}
\end{equation*}

Okay let's calculate $\fptheta f$ for this matrix!

\begin{equation*}
  \begin{split}
      \fptheta \left[f(\theta)\right]
    ={}&
      \fptheta \left[
        \left( \mtxA'_{2, 1}(\theta) \right)^2
        + \left( \mtxA'_{1, 2}(\theta) \right)^2
      \right]
    \\
    ={}&
      4
      \mtxA'_{2, 1}(\theta)
      \fptheta \left[ \mtxA'_{2, 1}(\theta) \right] \\
    \\
      \mtxA'_{2, 1}(\theta)
    ={}&
      2 \left( \cos^2\theta - \sin^2\theta \right) \\
      \mtxA'_{2, 1}(0)
    ={}&
        2 \left( \cos^2 0 - \sin^2 0 \right) \\
    ={}&
      2 \\
    \\
      \fptheta \left[ \mtxA'_{2, 1}(\theta) \right]
    ={}&
      \fptheta \left[
        2 \left( \cos^2\theta - \sin^2\theta \right)
      \right] \\
    ={}&
      2 \big( -2\cos\theta \sin\theta - 2\sin\theta\cos\theta \big) \\
    ={}&
      -8 \cos\theta \sin\theta \\
      \fptheta \left[ \mtxA'_{2, 1}(0) \right]
    ={}&
      -8 \cos 0 \sin 0 \\
    ={}&
      0 \\
    \\
      \fptheta \left[f(0)\right]
    ={}&
      4
      \mtxA'_{2, 1}(0)
      \fptheta \left[ \mtxA'_{2, 1}(0) \right] \\
    ={}&
      4 \cdot 2 \cdot 0 \\
    ={}& 0
  \end{split}
\end{equation*}

All our calculations check out with what we've done before. And we know
that, because this is a 2-by-2 real symmetric matrix, it must be
diagonalizable. And still the first partial is zero\dots

Well, the solution is clear: what is the story with the \textit{second}
partial?

\begin{equation*}
  \begin{split}
      \fpthetax \left[f(\theta)\right]
    ={}&
      \fptheta \Big[ \fptheta \left[f(\theta)\right] \Big] \\
    ={}&
      \fptheta \Big[
        4
        \mtxA'_{2, 1}(\theta)
        \fptheta \left[ \mtxA'_{2, 1}(\theta) \right]
      \Big] \\
    ={}&
      4 \Big[
        \fptheta \left[ \mtxA'_{2, 1}(\theta) \right]
        \fptheta \left[ \mtxA'_{2, 1}(\theta) \right]
        +
        \mtxA'_{2, 1}(\theta)
        \fpthetax \left[ \mtxA'_{2, 1}(\theta) \right]
      \Big] \\
    \\
      \fpthetax \left[ \mtxA'_{2, 1}(\theta) \right]
    &=
      \fptheta \Big[ -8 \cos\theta \sin\theta \Big] \\
    &= -8 \left( \cos^2\theta - \sin^2\theta \right) \\
      \fpthetax \left[ \mtxA'_{2, 1}(0) \right]
    &= -8 \left( \cos^2 0 - \sin^2 0 \right) \\
    &= -8 \\
    \\
      \fpthetax \left[f(0)\right]
    ={}&
      4 \Big[
        \fptheta \left[ \mtxA'_{2, 1}(0) \right]
        \fptheta \left[ \mtxA'_{2, 1}(0) \right]
        +
        \mtxA'_{2, 1}(0)
        \fpthetax \left[ \mtxA'_{2, 1}(0) \right]
      \Big] \\
    ={}&
      4 \Big[
        \left(0 \cdot 0\right)
        +
        \left(2 \cdot -8\right)
      \Big] \\
    ={}& -64
  \end{split}
\end{equation*}

Wonderful. In conclusion, we see that indeed the second partial is
negative, which means that we are at a local maximum.

\section{Conclusion}

Let's recap what we have done.

\begin{enumerate}
  \item We proved (elsewhere, previously) that rotation of basis keeps
  symmetric matrices symmetric.

  \item We then defined an ``error'' function $f$ to minimize. The error
  is minimal (zero) precisely when all off-diagonal entries are zeroed.

  \item Since the space of rotations of $\mtxA$ is compact, and because
  $f$ is continuous, we know it achieves a minimum in this space.

  \item We were unsure whether the minimum of $f$ on the space of
  rotations is actually zero.

  \item We use the first derivative test to discover that for any
  optimal $\mtxA$, either $\mtxA_{i, j} = \mtxA_{j, i} = 0$ (as desired)
  OR $\mtxA_{i, i} = \mtxA_{j, j}$ (leaving open the possibility that
  $\mtxA_{i, j} \ne 0$).

  \item We then examined what the second derivative would be at a
  first-order critical point, in the undesired case that $\mtxA_{i, j}
  \ne 0$, and thus necessarily $\mtxA_{i, i} = \mtxA_{j, j}$.

  \item The second derivative test showed us this must be a local
  \textit{maximum}. Thus we can preclude this scenario at a global
  minimum.

  \item We have foreclosed any escape. A global minimum must have
  $\mtxA_{i, j} = \mtxA_{j, i} = 0$, else it would be possible to
  improve.
\end{enumerate}

Does this proof have any advantage over the much more succinct proof JRN
gave me?

This proof accords with an intuition that you can always iteratively
keep improving your matrix by doing little rotations. It shows that you
can never get stuck. Indeed, you can use gradient descent if you want.
So this proof is much more clearly tied to a notion of
\textit{procedure}, which is certainly related to intuition.

Second, the proof says that, if you believe things work in two
dimensions, you can just keep picking pairs of basis vectors to fix, and
do your best. Remember, you might ``disrupt'' previous work, but to the
extent you disrupt previous work, you're helping other work. That's what
we know from the cancelation of the $\mtxA_{k, i}, \mtxA_{k, j}$ terms.

Probably that would have been a simpler proof right there. You have this
iterative process now, and it must converge to an answer, so the answer
must exist\dots

One goal was to limit the magical use of symmetry. Here it is still used
somewhat: you use symmetry when you presume that rotation of basis
preserves symmetry. Likewise, you use symmetry when you assume that
fixing things in one subspace won't have an overall negative impact.

TODO: Come back and think more about whether this proof really does feel
less mysterious?

\end{document}
