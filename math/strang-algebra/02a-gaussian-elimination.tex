\section{Ch2: Solving Linear Equations}

A system of linear equations can be seen as a matrix. Each equation is
a coordinate in the new space. Each equation defines a row in the
matrix. It's how much each coordinate in the original space
contributes to the value of the second space.

If we view the problem like this, then the solution is to take the
target in the transformed space and find its inverse under the
transformation.

The typical way to find the solution is to perform \define{Gaussian
Elimination}. What this does is slowly multiply each side of $\mA\vx =
\vb$ by a set of basic transformations until $\mA$ is eliminated and
$\vb$ has been transformed to its inverse under $\mA$.

The operations are these:

\begin{enumerate}
  \item Scale a row.
  \item Add or subtract a multiple of a row from another.
  \item Swap rows.
\end{enumerate}

The algorithm is like this:

\begin{enumerate}
  \item Start with the first row. Scale so its initial entry is 1. (For
  each transformation to $\mA$, perform the same transformation to
  $\vb$).

  \item Subtract the appropriate amount from the other rows so that you
  eliminate the other entries in the first column.

  \item Move to the second row. Repeat the process: scale so that the
  second element in the second row is 1, and then subtract as needed
  from the other columns.

  \item The only problem is if you encounter the $i$-th row and it has
  0 at the $i$-th column. In that case, swap two rows.
\end{enumerate}

If you apply the transformations to $\vb$, you will end up with convert
it to its inverse. You may instead apply these transformations to $\mI$,
which will convert this into the inverse linear transformation.

\TODO{That's not quite correct}. If you perform 1-4, remember you're
decomposing $\mA$ into $\mL\mU$. And if you're performing the inverse
operations to $\mI$, you're forming $\mLinv$. But after this, you need
to sweep back up again in $\mU$ to clear out the entries above the
diagonal.

Note: the version where we transform $\mI$ can be seen as finding the
inverse for $n$ vectors (the columns) simultaneously. That's kind of
interesting: to see it as nothing more than a parallelized/vectorized
version of solving for one inverse.

You may end up in a situation where you have only zeros left in a
column, so that no pivot can help you. In that case, this is a
\emph{singular} matrix, and it doesn't have a proper inverse. We'll talk
about that later.
