He discusses that an orthogonal matrix is its own inverse.

He notes that because orthogonal matrices don't change lengths, they
can be numerically helpful.

He describes Gram-Schmidt, which just successively subtracts out
projections onto the previously considered vectors. You of course can
collect these in a matrix R to put them back in.

The QR decomposition gives us a way to solve least squares. In that
case:

\begin{nedqn}
  \parensinv{\mtxAtA}
\eqcol
  \parensinv{
    \parenstrans{\mQ\mR}
    \mQ\mR
   }
\\
\eqcol
  \parensinv{
    \mRt \mQt \mQ \mR
  }
\\
\eqcol
  \parensinv{\mRt\mR}
\end{nedqn}

So:

\begin{nedqn}
  \parensinv{\mAt\mA} \mAt \vy
\eqcol
  \parensinv{\mRt \mR}
  \parenstrans{\mQ\mR}
  \vy
\\
\eqcol
  \mRinv
  \parensinv{\mRt}
  \mRt
  \mQt
  \vy
\\
\eqcol
  \mRinv
  \mQt
  \vy
\end{nedqn}

If you already have $\mA$ in $\mQ\mR$ form this should take $n^2$ time.
It's $n^2$ to apply $\mQt \vy$, and it's easy to invert an upper
triangular matrix.
