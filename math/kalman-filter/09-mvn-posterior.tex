\section{Multivariate Normal Posterior Calculation}

Let's bring our posterior calculation for $\vecmu$ into the multivariate
world!

First, we assume a multivariate normal prior on $\vecmu$. That is:

\begin{nedqn}
  \pvmux
& \sim &
  \normal{\vmumux}{\covmtxmux}
\end{nedqn}

Next, we assume a multivariate normal observation model for $\vecx$:

\begin{nedqn}
  \cpvxmux
& \sim &
  \normal{\vmux}{\covmtxx}
\end{nedqn}

We now want to calculate the posterior density:

\begin{nedqn}
  \cpvmuxx
& \sim &
  \pvmux
  \cpvxmux
  \\
& = &
  \mnormaleq{\vmux}{\vmumux}{\covmtxmux}
  \\&&
  \quad\quad \mnormaleq{\vx}{\vmux}{\covmtxx}
\end{nedqn}

As ever, we may simply focus on what's inside the exponent, disregarding
leading constants.

\begin{nedqn}
  \log \cpvmuxx
& \sim &
  \mnormaldistance{\vmux}{\vmumux}{\covmtxmux}
  +
  \mnormaldistance{\vx}{\vmux}{\covmtxx}
  \\
& = &
  \vmuxdiff\tran
  \covmtxmux\invx
  \vmuxdiff
  +
  \vxdiff\tran
  \covmtxx\invx
  \vxdiff
  \\
& = &
    \vmux\tran \covmtxmux\invx \vmux
  -
    2 \vmumux\tran \covmtxmux\invx \vmux
  +
    \vmumux\tran \covmtxmux\invx \vmumux
  \\&&\quad\quad
  +
    \vx\tran \covmtxx\invx \vx
  -
    2 \vmux\tran \covmtxx\invx \vx
  +
    \vmux\tran \covmtxx\invx \vmux
  \nedcomment{note use of symmetry}%
  \\
\simcol
    \vmux\tran \covmtxmux\invx \vmux
  -
    2 \vmumux\tran \covmtxmux\invx \vmux
  -
    2 \vmux\tran \covmtxx\invx \vx
  +
    \vmux\tran \covmtxx\invx \vmux
  \\
  &&\nedcomment{ignore terms without $\vmux$}%
  \\
\eqcol
    \vmux\tran
    \parens{
      \covmtxmux\invx
      +
      \covmtxx\invx
    }
    \vmux
  -
    2
    \vmux\tran
    \parens{
      \covmtxmux\invx \vmux
      +
      \covmtx\subvx\invx \vx
    }
  \nedcomment{collect like terms}
\end{nedqn}

Wow. This looks like our favorite problem: completing the square. We
want to rewrite in this form:

\begin{nedqn}
  \vectransvec{
    \parensqrt{
      \covmtxmux\invx + \covmtxx\invx
    }
    \vmux
    -
    X
  }
\end{nedqn}

And to do this, we must have:

\begin{nedqn}
  \vmux
  \parensqrt{
    \covmtxmux\invx + \covmtxx\invx
  }
  X
\eqcol
  \vmux\tran
  \parens{
    \covmtxmux\invx \vmux
    +
    \covmtxx\invx \vx
  }
\\
  X
\eqcol
  \parensnegsqrt{
    \covmtxmux\invx + \covmtxx\invx
  }
  \vmux\invx
  %
  \vmux\tran
  \parens{
    \covmtxmux\invx \vmux
    +
    \covmtxx\invx \vx
  }
\\
\eqcol
  \parensnegsqrt{
    \covmtxmux\invx + \covmtxx\invx
  }
  \parens{
    \covmtxmux\invx \vmux
    +
    \covmtxx\invx \vx
  }
\end{nedqn}

This completes the square, and so we have:

\begin{nedqn}
  \log \cpvmuxx
\simcol
  \vectransvec{
    \parensqrt{
      \covmtxmux\invx + \covmtxx\invx
    }
    \vmux
    -
    X
  }
  \\
\eqcol
  \normsq{
    \parensqrt{
      \covmtxmux\invx + \covmtxx\invx
    }
    \vmux
    -
    {
      \parensnegsqrt{
        \covmtxmux\invx + \covmtxx\invx
      }
      \parens{
        \covmtxmux\invx \vmux
        +
        \covmtxx\invx \vx
      }
    }
  }
\\
\eqcol
  \normsq{
    \parensqrt{
      \covmtxmux\invx + \covmtxx\invx
    }
    %
    \sqbrackets{
      \vmux
      -
      {
        \parensinv{
          \covmtxmux\invx + \covmtxx\invx
        }
        \parens{
          \covmtxmux\invx \vmux
          +
          \covmtxx\invx \vx
        }
      }
    }
  }
\end{nedqn}

Wow oh wow. This is indeed in proper form! Let's write down our new
values:

\begin{nedqn}
  \covmtxp\subvmux
\eqcol
  \parensinv{
    \covmtxmux\invx
    +
    \covmtxx\invx
  }
\\
  \vmumuxp
\eqcol
  \covmtxp\subvmux
  \parens{
    \covmtxmux\invx \vmux
    +
    \covmtxx\invx \vx
  }
\end{nedqn}

%% TODO: Intuition for this weighted sum!

%% TODO: Maybe macros for the posterior parameters?
