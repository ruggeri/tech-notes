\newcommand{\mnormalc}[1][\covmtx]{
  \invf{
    \sqrt{
      \parens{\twopi}^n
      \norm{{#1}}^2
    }
  }
}

\newcommand{\mnormaleq}{
  \mnormalc
  \nexp{
    -\half
    \norm{
      \covmtx\negsqrt
      \vecx
    }
  }
}

\section{The Multivariate Normal Distribution}

\subsection{Standard Spherical Normal}

We will now extend our normal distribution to vectors over $\rn$. The
simplest thing is to consider the product of independent normal
distributions. We'll first consider the \define{standard spherical
normal}:

\begin{nedqn}
  \normal{\veczero}{\mtxI}
& \defeq &
  \prod_i
    \snormaleq[\vecx_i]
  \\
& = &
  \invf{
    \sqrt{\parens{\twopi}^n}
  }
  \nexp{
    -\half
    \sum_i
      \vecx_i^2
  }
  \\
& = &
  \invf{
    \sqrt{\parens{\twopi}^n}
  }
  \nexp{
    -\half
    \norm{\vecx}^2
  }
\end{nedqn}

This standard spherical normal is not so different than the
one-dimensional standard normal. The only thing that has changed is that
we take a multidimensional vector $\vecx$ and map it to its scalar norm
before squaring. Really, the multivariate normal is just like its
univariate counterpart, in that it really only cares about the distance
of a point $\vecx$ from the distribution's center.

\subsection{Stretching $\vecx$ by a Diagonal Matrix $\mtxD$}

Imagine that that $X$ is distributed per standard spherical normal. We
have the equation for density of points $\vecx$. Next question: what is
the density for a variable $Y = \mtxD X$? We're restricting ourself to
just considering a diagonal matrix $\mtxD$. That is, what is the density
at a point $\vecy = \mtxD\vecx$?

This involves a simple change of variables. Replace $\vecx_i$ with
$\frac{\vecy_i}{\mtxD\subii}$.

\begin{nedqn}
  \invf{
    \sqrt{\parens{\twopi}^n}
  }
  \nexp{
    -\half
    \sum_i
      \vecx_i^2
  }
& \mapsto &
  \invf{
    \prod_i
      \mtxD\subii
  }
  \invf{
    \sqrt{\parens{\twopi}^n}
  }
  \nexp{
    -\half
    \sum_i
      \parensq{
        \frac{
          \vecy_i
        }{
          \mtxD\subii
        }
      }
  }
  \\
& = &
  \mnormalc[\mtxD]
  \nexp{
    -\half
    \norm{
      \mtxDinv
      \vecy
    }^2
  }
\end{nedqn}

Notice that because I want a \emph{density}, which must change because
the entire space is being stretched by $\mtxD$, I have multiplied by
$\norm{\mtxD}\invx$. Again, note how I have vectorized here. What is the
role of $\mtxDinv$? Yes, it does map $\vecy$ back to $\vecx$. But
because we only need $\vecx$ in order to take its norm, the real point
is in mapping $\vecy$ back to a vector (any vector!) with length
$\norm{\vecx}$. Again, the normal distribution is all about distance.

\subsection{Stretching $\vecx$ by $\mtxDQ$}

If $\vecx$ is distributed per $\normal{\veczero}{\mtxI}$, then the
density for $\mtxDQ \vecx$ is exactly equal to the distribution over
$\mtxD \vecx$.

The reason is simple:

\begin{nedqn}
  \parensinv{\mtxDQ} \vecy
& = &
  \mtxQt \mtxDinv \vecy
  \\
\Rightarrow
  \norm{
    \parensinv{\mtxDQ} \vecy
  }
& = &
  \norm{\mtxQt \mtxDinv \vecy}
  \\
& = &
  \norm{\mtxDinv \vecy}
\end{nedqn}

It makes sense. Once we apply $\mtxDinv$ to $\vecy$ to shrink it back
to original norm, it no longer matters for us to rotate back by
$\mtxQt$. The \emph{spherical} normal doesn't care about that!

\subsection{Stretching $\vecx$ by $\mtxA$}

Let's now consider stretching $\vecx$ by a general matrix $\mtxA$. Every
matrix $\mtxA$ can be written in singular value decomposition form:
$\mtxUDQt$.

We've already argued why we can ignore the initial rotation by $\mtxQt$.
So really all that matters is inverting $\mtxUD$. For our purposes,
$\norm{\parensinv{\mtxUD} \vecy}$ is as good as $\norm{\mtxAinv \vecy}$.

\subsection{Covariance Matrix}

We've seen that lots of transformations ``mutate'' the sense of distance
in the same way. Specifically: take any two transformations $\mtxA$,
$\mtxA'$ that are equivalent except for an initial rotation. They result
in the same sense of distance transformation when we go to invert them.
That is:

\begin{nedqn}
  \norm{
    \parensinv{\mtxUDQt}
    \vecy
  }
& = &
  \norm{
    \parensinv{\mtxUD \mtxQ\ptran}
    \vecy
  }
\end{nedqn}

We want to collect up all the stretchings of $\vecx$, by any matrix
$\mtxA$, and call this the family of normal distributions. How should we
notate them? In particular, two different $\mtxA$s might result in the
same distribution. How do we capture what matters, and disregard
redundancy?

If $\mtxA = \mtxUDQt$, then what matters is how to ``unrotate'' the
$\mtxU$ part and how to ``unshrink'' the $\mtxD$ part. We want to
parameterize by something like

\begin{nedqn}
  \mtxDinv \mtxUt
\end{nedqn}

But how can we find this given just $\mtxA$? We can use the following
fact:

\begin{nedqn}
  \mtxA\mtxAt
& = &
  \parens{
    \mtxUDQt
  }
  \parens{
    \mtxUDQt
  }\tran
  \\
& = &
  \parens{
    \mtxUDQt
  }
  \parens{\mtxQDUt}
  \\
& = &
  \mtxU \mtxD^2 \mtxUt
\end{nedqn}

We name this quantity the \define{covariance matrix}: $\covmtx$. It is
almost exactly what we want. Multiplication by $\mtxUt$ rotates from the
$\vecy$ coordinate system back to the coordinate system after the
initial rotation of $\vecx$ by $\mtxQt$. We don't particularly care
which $\mtxQ$, so long as we have returned to it. Next, we ``double
stretch.'' This does the stretching along the same axes that $\mtxA$ did
the original stretching. Last, we rotate back to the $\vecy$ coordinate
system As noted, this hardly matters (for now). For now, it is a
convenient and more importantly unique choice.

As explained, this will take a vector in $\vecy$ space and double
stretch along the stretching axis defined by $\mtxQt$. But we don't want
to do that. We want to instead \emph{unstretch}. But the solution is
simple: take $\covmtx\negsqrt$. This is equal to:

\begin{nedqn}
  \covmtx\negsqrt
& = &
  \mtxU \mtxDinv \mtxUt
\end{nedqn}

For this reason, if we take a spherical normal and stretch by the matrix
$\mtxA$, we notate this as $\normal{\veczero}{\mtxAAt}$. Or more simply:

\begin{nedqn}
  \normal{\veczero}{\covmtx}
& \defeq &
  \mnormaleq
\end{nedqn}

\subsection{Series of Linear Transformations}

Consider if we have $\vecx \sim \normal{\veczero}{\mtxI}$, and we want
to find the distribution on $\mtxBA \vecx$. As ever, we'll use the SVD
notation:

\begin{nedqn}
  \mtxA
& \defeq &
  \mtxU\submA \mtxD\submA \mtxQt\submA
  \\
  \mtxB
& \defeq &
  \mtxU\submB \mtxD\submB \mtxQt\submB
\end{nedqn}


From the above work, finding $\covmtxBA$ is simple. We simply calculate
the covariance matrix for $\mtxBA$ in the usual way:

\begin{nedqn}
  \covmtxBA
& = &
  \parens{\mtxBA}
  \parens{\mtxBA}\tran
  \\
& = &
  \mtxB \covmtxA \mtxBt
\end{nedqn}

Notice that you don't get $\covmtxBA$ simply through some combination of
$\covmtxA$ and $\covmtxB$. Why? The same $\mtxB$ will affect different
covariance matrices $\covmtxA$ in different ways.

This makes sense. In particular, $\covmtxB$ no longer contains any
information about $\mtxQt\submB$. And $\mtxQt\submB$ now does matter.
Why? Because in $\vecy = \mtxA\vecx$ space, not all rotations of $\vecy$
have equal density. We need to make sure we do rotate by $\mtxQ\submB$,
so that we can properly continue the process by applying $\covmtxA$ in
the middle of the $\mtxB, \mtxBt$ ``sandwich.''
