\newcommand{\mnormalc}[1][\covmtx]{
  \invf{
    \sqrt{
      \parens{\twopi}^n
      \norm{{#1}}
    }
  }
}

\newcommand{\mnormaleq}{
  \mnormalc
  \nexp{
    -\half
    \norm{
      \covmtx^{-1/2}
      \vecx
    }
  }
}

\section{The Multivariate Normal Distribution}

\subsection{Standard Spherical Normal}

We will now extend our normal distribution to vectors over $\rn$. The
simplest thing is to consider the product of independent normal
distributions. We'll first consider the \define{standard spherical
normal}:

\begin{nedqn}
  \normal{\veczero}{\mtxI}
& \defeq &
  \prod_i
    \snormaleq[\vecx_i]
  \\
& = &
  \invf{
    \sqrt{\parens{\twopi}^n}
  }
  \nexp{
    -\half
    \sum_i
      \vecx_i^2
  }
  \\
& = &
  \invf{
    \sqrt{\parens{\twopi}^n}
  }
  \nexp{
    -\half
    \norm{\vecx}^2
  }
\end{nedqn}

This standard spherical normal is not so different than the
one-dimensional standard normal. The only thing that has changed is that
we take a multidimensional vector $\vecx$ and map it to its scalar norm
before squaring. Really, the multivariate normal is just like its
univariate counterpart, in that it really only cares about the distance
of a point $\vecx$ from the distribution's center.

\subsection{Stretching $\vecx$ by a Diagonal Matrix $\mtxD$}

Imagine that that $X$ is distributed per standard spherical normal. We
have the equation for density of points $x$. Next question: what is the
density for a variable $Y = \mtxD X$? We're restricting ourself to just
considering a diagonal matrix $\mtxD$. That is, what is the density at a
point $\vecy = \mtxD\vecx$?

This involves a simple change of variables. Replace $\vecx_i$ with
$\frac{\vecy_i}{\mtxD_{i, i}}$.

\begin{nedqn}
  \invf{
    \sqrt{\parens{\twopi}^n}
  }
  \nexp{
    -\half
    \sum_i
      \vecx_i^2
  }
& \mapsto &
  \invf{
    \prod_i
      \mtxD_{i, i}
  }
  \invf{
    \sqrt{\parens{\twopi}^n}
  }
  \nexp{
    -\half
    \sum_i
      \parens{
        \frac{
          \vecy_i
        }{
          \mtxD_{i, i}
        }
      }^2
  }
  \\
& = &
  \invf{
    \sqrt{
      \parens{\twopi}^n
      \norm{\mtxD}^2
    }
  }
  \nexp{
    -\half
    \sum_i
    \norm{
      \mtxDinv
      \vecy
    }^2
  }
\end{nedqn}

Notice that because I want a \emph{density}, which must change because
the entire space is being stretched by $\mtxD$, I have multiplied by
$\norm{\mtxD}\invx$. Again, note how I have vectorized here. What is the
role of $\mtxDinv$? Yes, it does map $\vecy$ back to $\vecx$. But
because we only need $\vecx$ in order to take its norm, the real point
is in mapping $\vecy$ back to a vector (any vector!) with length
$\norm{\vecx}$. Again, the normal distribution is all about distance.

\subsection{Stretching $\vecx$ by $\mtxDQ$}

If $\vecx$ is distributed per $\normal{\veczero}{\mtxI}$, then the
density for $\mtxDQ \vecx$ is exactly equal to the distribution over
$\mtxD \vecx$.

The reason is simple:

\begin{nedqn}
  \parensinv{\mtxDQ} \vecy
& = &
  \mtxQt \mtxDinv \vecy
  \\
\Rightarrow
  \norm{
    \parensinv{\mtxDQ} \vecy
  }
& = &
  \norm{\mtxQt \mtxDinv \vecy}
  \\
& = &
  \norm{\mtxDinv \vecy}
\end{nedqn}

It makes sense. Once we apply $\mtxDinv$ to $\vecy$ to shrink it back
to original norm, it no longer matters for us to rotate back by
$\mtxQt$. The \emph{spherical} normal doesn't care about that!

\subsection{Stretching $\vecx$ by $\mtxA$}

Let's now consider stretching $\vecx$ by a general matrix $\mtxA$. Every
matrix $\mtxA$ can be written in singular value decomposition form:
$\mtxUD \mtxQt$.

We've already argued why we can ignore the initial rotation by $\mtxQt$.
So really all that matters is inverting $\mtxUD$. For our purposes,
$\norm{\parensinv{\mtxUD} \vecy}$ is as good as $\norm{\mtxAinv \vecy}$.

\subsection{Covariance Matrix}

We've seen that lots of transformations ``mutate'' the sense of distance
in the same way. Specifically: take any two transformations $\mtxA$,
$\mtxA'$ that are equivalent except for an initial rotation. They result
in the same sense of distance transformation when we go to invert them.
That is:

\begin{nedqn}
  \norm{
    \parensinv{\mtxUDQt}
    \vecy
  }
& = &
  \norm{
    \parensinv{\mtxUD \mtxQ\ptran}
    \vecy
  }
\end{nedqn}

We want to collect up all the stretchings of $\vecx$, by any matrix
$\mtxA$, and call this the family of normal distributions. How should we
notate them? In particular, two different $\mtxA$s might result in the
same distribution. How do we capture what matters, and disregard
redundancy?

If $\mtxA = \mtxUDQt$, then what matters is how to ``unrotate'' the
$\mtxU$ part and how to ``unshrink'' the $\mtxD$ part. We want to
parameterize by something like

\begin{nedqn}
  \mtxDinv \mtxUt
\end{nedqn}

But how can we find this given just $\mtxA$? We can use the following
fact:

\begin{nedqn}
  \mtxA\mtxAt
& = &
  \parens{
    \mtxUDQt
  }
  \parens{
    \mtxUDQt
  }\tran
  \\
& = &
  \parens{
    \mtxUDQt
  }
  \mtxQ \mtxD \mtxUt
  \\
& = &
  \mtxU \mtxD^2 \mtxUt
\end{nedqn}

We name this quantity the \define{covariance matrix}: $\covmtx$. It is
almost exactly what we want. Multiplication by $\mtxUt$ rotates from the
$y$ coordinate system back to the coordinate system after the initial
rotation by $\mtxQt$. We don't particularly care which $\mtxQ$, so long
as we have returned to it. Next, we ``double stretch.'' This does the
stretching along the same axes that $\mtxA$ did the original stretching.
Last, we rotate back to the $y$ coordinate system As noted, this hardly
matters (for now). For now, it is a convenient and more importantly
unique choice.

As explained, this will take a vector in $\vecy$ space and double
stretch along the stretching axis defined by $\mtxQt$. But we don't want
to do that. We want to instead \emph{unstretch}. But the solution is
simple: take $\covmtx^{-1/2}$. This is equal to:

\begin{nedqn}
  \covmtx^{-1/2}
& = &
  \mtxU \mtxDinv \mtxUt
\end{nedqn}

For this reason, if we take a spherical normal and stretch by the matrix
$\mtxA$, we notate this as $\normal{\veczero}{\mtxAAt}$. Or more simply:

\begin{nedqn}
  \normal{\veczero}{\covmtx}
& \defeq &
  \mnormaleq
\end{nedqn}

\subsection{Series of Linear Transformations}

Consider if we have $\vecx \sim \normal{\veczero}{\mtxI}$, and we want
to find the distribution on $\mtxBA \vecx$. As ever, we'll use the SVD
notation:

\begin{nedqn}
  \mtxA
& \defeq &
  \mtxU\submA \mtxD\submA\invx \mtxQt\submA
  \\
  \mtxB
& \defeq &
  \mtxU\submB \mtxD\submB\invx \mtxQt\submB
\end{nedqn}


From the above work, finding $\covmtxBA$ is simple. We simply calculate
the covariance matrix for $\mtxBA$ in the usual way:

\begin{nedqn}
  \covmtxBA
& = &
  \parens{\mtxBA}
  \parens{\mtxBA}\tran
  \\
& = &
  \mtxB \covmtxA \mtxBt
\end{nedqn}

Notice that you don't get $\covmtxBA$ through some combination of
$\covmtxA$ and $\covmtxB$. Why? Consider the series of steps

\begin{nedqn}
  \vecx
\mapsto
  \parens{\vecy = \mtxA\vecx}
\mapsto
  \parens{\vecz = \mtxB\vecy = \mtxBA\vecx}
\end{nedqn}

First, we know that $\covmtxA^{-1/2} \vecy$ unstretches along the axes
$\mtxQt\submA$ (without explicitly remembering what that was), and is
otherwise careful not to spuriously rotate $\vecy$. That's what is nice
about $\covmtxA^{-1/2} = \mtxU\submA \mtxD\submA\invx \mtxUt\submA$,
versus any other rotation $\mtxU' \mtxD\submA\invx \mtxUt\submA$.

Specifically, it makes it possible to calculate $\covmtxBA$ from just
$\mtxB$ and $\covmtxA$. TODO: the algebra of this is very clear, but is
there a more intuitive explanation for what it means to multiply on both
sides by $\mtxB, \mtxBt$?

Just as we must not forget the final rotation by $\mtxU\submA$ in
$\covmtxA$, we must not forget $\mtxQt\submB$. It's okay to forget
$\mtxQt\submA$ since there we are just rotating $\vecx$, and the density
at $\mtxQ' \vecx$ is the same for any rotation $\mtxQ'$.

But it is \emph{not} okay to forget $\mtxQt\submB$. This controls
rotation in $\vecy$ space, and in that space not all $\mtxQ' \vecy$ have
the same density.
