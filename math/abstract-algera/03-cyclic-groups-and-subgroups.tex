\section{Cyclic Groups and Subgroups}

\subsection{Definition}

\begin{definition}
  An element $g \in G$ is a \define{generator} if every element of the
  group $a$ is equal to some $g^i$. We say that the group $G$ is
  \define{cyclic}.
\end{definition}

\begin{remark}
  Finite cyclic groups of the same order are isomorphic. In particular
  we can always consider $\Zmodn$ (with addition as the group
  operation). Note that zero is the identity (its inverse is itself).

  Infinite cyclic groups are isomorphic to $\Z$.
\end{remark}

\subsection{Order Of Elements}

\begin{proposition}
  If $g$ generates $G$, and $k$ divides $|G|$, then the order of $g^k$
  is of course $|G|/k$. More generally for any $k$: $|g^k|$ is
  $\frac{|g|}{\gcd(|g|, k)}$.
\end{proposition}

\begin{proof}
  Consider $g^k$ raised to the $\frac{|g|}{\gcd(|g|, k)}$ power. This is
  $g$ to the power of $\frac{|g|k}{\gcd(|g|, k)}$. This is the least
  common multiple of $|g|, k$. Thus it is $g^{|g|}$ to the something.
  Which means it is one to the something. Which means it is $1$.

  Because $\frac{|g|k}{\gcd(|g|, k)}$ is the \emph{least} common
  multiple of $|g|, k$, this is the first time that $g^k$ will wrap to a
  power of $|g|$. Thus $|g^k| = \frac{|g|}{\gcd(|g|, k)}$ as desired.
\end{proof}

\begin{remark}
  If $k$ is coprime with $|g|$, then $|g^k| = |G|$. Thus $g^k$ generates
  the entire group. There are thus $\varphi(|G|)$ generators of a cyclic
  group $G$.
\end{remark}

\begin{remark}
  Note that $|g^k|$ always divides the order of the cyclic group.
\end{remark}

\subsection{Subgroups of Cyclic Groups}

\begin{theorem}
  Every subgroup of a cyclic subgroup is cyclic.
\end{theorem}

\begin{proof}
  Choose any two $g^a, g^b \in H$. I want to show that \emph{both}
  elements are generated by a common $g^k$. If I do this, I can then
  consider $g^k, g^c$, to find a common generator. Repeating, I will
  find a minimal $g^k$ that generates the entire group.

  Let's begin. Now, if $a$ divides $b$ (or vice versa), we are done.
  Otherwise, consider $\gcd(a, b)$. By Bezout's lemma, we can always
  find $x, y$ such that $xa - yb = \gcd(a, b)$. Thus, by closure of a
  subgroup, we know that $g^{\gcd(a, b)}$ is in the subgroup.

  But this implies that there exists $k_a, k_b$ such that $g^{\gcd(a,
  b)}$ can be exponentiated to yield $g^a, g^b$. Thus $g^a, g^b$ are
  both generated by this element.
\end{proof}

\begin{remark}
  We thus see that for any set of $g^{a_i}$, the subgroup generated by
  the collection is exactly equal to the cyclic subgroup generated by
  $g^k$, where $k = \gcd\parens{\setof{a_i}}$.
\end{remark}

\begin{remark}
  We can say more than this! By Bezout's lemma again, there exists $x,
  y$ such that $xk - y|G| = \gcd\parens{k, |G|}$. Note that Bezout shows
  that you can never choose $x, y$ to get \emph{smaller} than
  $\gcd\parens{k, |G|}$.

  This shows that every subgroup is generated by some $g^{k'}$ where
  $k'$ \emph{divides} $|G|$.
\end{remark}

\begin{theorem}
  By the preceding remark, we've shown that subgroups of a cyclic group
  $|G|$ can be put into \emph{bijection} with divisors of $|G|$.

  There is thus a \emph{unique} subgroup of size $k$ for each divisor of
  $|G|$.
\end{theorem}

\subsection{Subgroups Generated By Sets}

\begin{definition}
  Consider an acyclic group $G$. Consider any subset $A$. You can
  \define{generate a subgroup from $A$} by taking all finite products of
  members of $A$ (and their inverses).
\end{definition}

\begin{remark}
  The book discusses how there can be many more distinct ``words''
  formed from $A$ if $G$ is \emph{non-abelian}.
\end{remark}

\begin{remark}
  The book describes drawing a ``lattice'' of subgroups of $G$. This
  lattice is a DAG where the vertices are subgroups of $G$, and the
  edges are from $A$ to $B$ if (a) $A \subgroup B$, but (b) there is no
  $C$ such that $A \subgroup C \subgroup B$.

  Of course isomorphic groups have the same lattice. However, apparently
  some non-isomorphic groups \emph{also} have the same lattice!
\end{remark}

\subsection{Notes On Factoring Cyclic Groups}

\begin{remark}
  Here I go into my own exploration apart from the book.
\end{remark}

\begin{definition}
  The \define{join} of two subgroups $A, B$ consists of all products
  $ab$. If $A, B$ are disjoint, then the join has size $|A||B|$. (I'm
  assuming abelian I guess?).

  A group $G$ is \define{factored} if I can separate it into disjoint
  subgroups whose join is $G$.
\end{definition}

\begin{theorem}
  A cyclic group $G$ of size $\prod_i p_i^{k_i}$ can be factored into
  disjoint subgroups $G_i$ of size $p_i^{k_i}$.
\end{theorem}

\begin{proof}
  We've seen that for a cyclic group $G$, there is exactly \emph{one}
  cyclic subgroup of size $k$ for each $k$ dividing $|G|$.

  So let $G_i$ be the subgroup of size $p_i^{k_i}$. The intersection of
  any two $G_i, G_j$ must be trivial. Any subgroup of these must have
  size dividing both $|G_i|, |G_j|$, which are necessarily coprime. Thus
  the $G_i$ are disjoint.

  Each $G_i$ is generated by some element $g_i$. I claim that $g =
  \prod_i g_i$ is a generator of $G$. Why? Because the $|g|$ is the
  least common multiple of $p_i^{k_i}$. Thus $|g| = |G|$, and so $g$
  generates the whole group.
\end{proof}

\begin{remark}
  Note that I don't need to assume $G$ is cyclic. Take all elements with
  order dividing $p_i^{k_i}$ and group them as $G_i$. Each $G_i$ is
  clearly a subgroup. And each $G_i$ is clearly disjoint. And the join
  must still be all of $G$.

  You may note: chose a minimal \emph{set} of generators $G'_i$ for each
  $G_i$. Then take all products $\prod_i g_i$ for $g_i \in G'_i$. This
  generates $G$. I won't prove it, but of course if $G'_i$ has just one
  element $g_i$, we're back to the cyclic case.
\end{remark}

\begin{theorem}
  A cyclic group $G$ of size $p^k$ cannot be further factored into
  smaller subgroups that join together back to $G$.
\end{theorem}

\begin{proof}
  Consider any two proper subgroups $A, B$. I claim that one contains
  the other.

  WLOG, let's assume that $|A| \leq |B|$. Then I claim that every $ab$
  has order dividing $|B|$. Why? Because $|A|, |B|$ are both powers of
  $p_i$. So the smaller power must divide the larger one.

  Since every $ab$ has order dividing $|B|$, the join has size at most
  $|B|$. Which means the join is exactly $B$. Which means that $A$ must
  have already been contained in $B$.

  But that means our task is hopeless. We cannot factor into disjoint
  subgroups; there is simply a series of subgroups each contained in the
  other Matryoshka style. And the join of these subgroups never gives
  anything new.
\end{proof}

\begin{theorem}
  An \emph{acyclic} group of size $p^k$ is decomposable.
\end{theorem}

\begin{proof}
  Consider a maximal cyclical subgroup $G_1$ generated by $g_1$. This
  has size $p^{k_1}$.

  Next, let's partition $G$ into equivalence classes. We'll say that two
  elements $g, g'$ are equivalent if $g = g_1^i g'$. That is, we are
  saying that two elements are equivalent if they differ only by a
  factor of some element of $G_1$. There are $p^{k_2} = p^{k - k_1}$
  equivalence classes, each containing $p^{k_1}$ elements.

  I claim that by careful selection from the equivalence classes, we can
  construct a disjoint subgroup $G_2$. I won't go into precisely how.
  Presumably we'll learn when we cover quotient groups.

  Anyway, I claim that you have now decomposed $G$. You have two
  disjoint subgroups, so their join should be $G$.
\end{proof}

\begin{remark}
  Since $G$ is acyclic, perhaps $G_2$ is acyclic (since $G_1$ is cyclic
  by assumption). In this case $G_2$ can be further factored.

  Or perhaps $G_2$ is \emph{also} cyclic. In that case it must
  \emph{not} be decomposable. Note that in this case the join of $G_1,
  G_2$ is not as simple as taking the two generators $g_1, g_2$ and
  forming $g = g_1 g_2$ that generates all of $G$.
\end{remark}

\begin{theorem}[Fundamental Theorem of Finite Abelian Groups]

  Every finite abelian group can be uniquely factored into a product of
  prime power cyclic groups.
\end{theorem}

\begin{proof}
  We've proven this by the above work.
\end{proof}

\begin{corollary}
  Every finite group of prime size is cyclic.
\end{corollary}

\begin{remark}
  How many abelian groups are there of size $p^k$? We know that this
  must decompose into a product of cyclic groups of size $p^i$. How many
  distinct ways are there to ``partition'' $k$ into non-negative
  integers $i$? This is called the \define{partition function}, which is
  denoted $p(k)$.

  Apparently there is no (known?) closed form for $p(k)$. Though it can
  be calculated through a recurrence relation.
\end{remark}

\begin{remark}
  The number of abelian groups of size $\prod_i p^{k_i}$ is thus equal
  to $\prod_i p(k_i)$.
\end{remark}

\clearpage
\section{Finite Fields}

\subsection{Definitions}

\begin{remark}
  These notes are based off of:
  \url{https://web.stanford.edu/class/ee392d/Chap7.pdf}. Those notes are
  part of EE392d, a coding theory course at Stanford. This chapter is
  from the reading notes for that class.
\end{remark}

\begin{definition}
  A \define{monoid} is almost a group. The difference is that not every
  element need be invertible. The integers (either positive or
  non-negative) are a monoid with respect to multiplication.

  Like groups/abelian groups, monoids may be either commutative or
  non-commutative.
\end{definition}

\begin{definition}
  A \define{ring} is a set with $+, \cdot$ operations. The set is an
  abelian group with respect to $+$ (the identity is written 0).

  The group is a \define{monoid} with respect to multiplication. That
  is: multiplication is associative, and there is an identity 1.
  Elements, (even non-zero elements) may not be invertible.

  The distributive law is required to hold: $a(b + c) = ab + ac$.
\end{definition}

\begin{definition}
  A \define{commutative ring} is one in which the multiplication
  operation commutes. When a ring has a multiplication which is not
  commutative, we will emphasize this by calling it a
  \define{non-commutative ring}. When we just write ``ring,'' we'll
  assume it is a commutative ring.
\end{definition}

\begin{definition}
  A \define{field} is a commutative ring where every non-zero element
  has an inverse.
\end{definition}

\begin{definition}
  We can talk about a ``non-commutative field.'' When the multiplication
  operation is non-commutative, we call this a \define{division ring} or
  \define{skew field}. But we won't study these now.

  Wedderburn's little theorem (which I have not studied) proves that all
  finite skew fields are in fact proper commutative fields. The only
  ``true'' skew fields are infinite.
\end{definition}

\subsection{Every field with $p$ elements is isomorphic to $\Zmodp$}

\begin{theorem}
  $\Zmodn$ is a field exactly when $n$ is prime.
\end{theorem}

\begin{proof}
  The addition and multiplication operations are both commutative. The
  distributive law holds. These are facts on the integer operations, in
  terms of which $\Zmodn$ is defined.

  Both a 0 and 1 elements exist. Additive inverses always exist. Last,
  note that for all $x \in \Zmodn$, $x\Zmodn$ is a permutation when $n$
  is prime. Thus all $x$ are invertible when $n$ is prime. But this is
  not so when $n$ is composite.
\end{proof}

\begin{theorem}
  The \define{integers} of a \emph{finite} field $\Field$ are the sums
  generated by repeated addition of 1: $0, 1, 1+1, 1+1+1, \ldots$. The
  integers of a \emph{finite} field are always themselves a subfield.
\end{theorem}

\begin{proof}
  Presuming that $\Field$ is finite, the set of integers of $\Field$ is
  also finite, and must wrap back to 0 eventually. (This is how we use
  our important presumption that $\Field$ is finite.) So the integers
  are a cyclic group generated by $1$ with respect to addition.

  Next question: are the integers of $\Field$ a \define{subfield}? We
  consider the product $ij$ of integers $i, j$. But we can use the
  distributive law:

  \begin{nedqn}
    ij
  \eqcol
    \parens{1_1 + 1_2 + \ldots + 1_i} \parens{1_1 + 1_2 + \ldots + 1_j}
  \\
  \eqcol
    1_1\parens{1_1 + 1_2 + \ldots + 1_j}
    + \ldots +
    1_2\parens{1_1 + 1_2 + \ldots + 1_j}
  \end{nedqn}

  \noindent
  This shows that $ij$ is just the result of adding $1$ to itself $ij$
  times. Which shows that the integers are isomorphic to $\Zmodn$. If
  $n$ were not prime, then multiplication by some $i$ in $\Zmodn$ is not
  injective. In this case, $\Zmodn$ is not a field, and neither can
  $\Field$ be.

  Since of course we've assumed $\Field$ \emph{is} a field, we know that
  $n$ must be prime and $\Zmodn$ must be a subfield.
\end{proof}

\begin{corollary}
  There are always a prime number of integers. We call the number of
  integers the \define{characteristic} of the field $\Field$.
\end{corollary}

\begin{corollary}
  If the characteristic of $\Field$ is $p$, then any field element $x$
  added to itself $p$ times will always be zero.
\end{corollary}

\begin{proof}
  \begin{nedqn}
    x_1 + x_2 + \ldots + x_p
  \eqcol
    \parens{1_1 + 1_2 + \ldots + 1_p} x
  \\
  \eqcol
    0x
  \\
  \eqcol
    0
  \end{nedqn}
\end{proof}

\begin{theorem}
  Every finite field of $p$ elements is isomorphic to $\Zmodp$. We
  denote this $\Field_p$ or even more commonly as $\GF{p}$. We call this
  the \define{Galois Field} of $p$ elements.
\end{theorem}

\begin{proof}
  We know that for a finite field $\Field$ with $p$ elements, the
  integers are a subfield. But Lagrange implies that the number of
  integers must divide $p$. Since the number of integers must always be
  at least two (the integers always contains 0 and 1), the number of
  integers must not trivially divide $p$. Thus the number of integers is
  in fact $p$, and thus $\Field \cong \GF{p}$.
\end{proof}

\subsection{Every finite field must have $p^k$ elements}

\begin{theorem}
  A finite field $\F$ must have $p^k$ elements (for some prime $p$ and
  any $k > 0$).
\end{theorem}

\begin{proof}
  Consider the additive group structure of $\F$. By the Fundamental
  Theorem of Finite Abelian Groups, this is isomorphic to $\prod
  \Zmod{p_i^{k_i}}$.

  One of these subgroups represents the integers of the field. WLOG
  presume this is $\Zmod{p_1^{k_1}}$. In fact, we've said the integers
  must have prime order $p_1$ with $k_1 = 1$.

  Now consider any other factor subgroup $\Zmod{p_i^{k_i}}$. I claim
  that $i = p_1$ and $k = 1$. Why? Because take a generator $x$ of the
  factor additive subgroup. We know that $\parens{1_1 + \ldots +
  1_{p_1}} x = 0$, because $p_1$ is the characteristic of the field. But
  then the (additive) order of $x$ must divide $p$. And because $p$ is
  prime, this can only happen if the order is exactly $p_1$.

  Thus all factor subgroups have the same order: $p$. Thus the field
  overall has exactly $p^k$ elements.
\end{proof}

\begin{corollary}
  No set of $ab$ elements (for $a, b$ coprime) can be a field.
\end{corollary}

\begin{remark}
  We know that if a field $\F$ has $p^k$ elements, it must have additive
  structure isomorphic to $x_0 \times \ldots \times x_{k-1}$. In fact,
  we know that the $x_i$ are really just elements of $\Zmodp$, where $p$
  is the characteristic of the finite field. We will write them as $f_0,
  \ldots f_{k-1}$.

  The question is what kind of multiplicative structures will be
  compatible with the distributive law. This will determine what the
  various fields of order $p^k$ are.

  It turns out that for each pair $p, k$, there is exactly one field
  with order $p^k$. That's what we will prove.
\end{remark}

\begin{remark}
  \TODO{It seems that, because the integers must always have order $p$,
  it's not possible to have a field of polynomials over a field of $p^k$
  elements?}
\end{remark}

\subsection{Polynomial Rings}

\begin{definition}
  A polynomial over $\Field$ is an expression $f_0 + f_1x + \ldots + f_m
  x^m$. The ``coefficients'' $f_i$ are taken from $\Field$. We can add
  polynomials coordinate-wise.

  We define the standard multiplication of polynomials by
  \define{convolution}. For two polynomials $f, g$, their product is a
  new polynomial $h$ where the coefficients of $h$ are given by:

  \begin{nedqn}
    h_i
  \eqcol
    \sum_{j = 0}^i
    f_j
    g_{i - j}
  \end{nedqn}
\end{definition}

\begin{proposition}
  We denote the space of all polynomials over a field $\Field$ as
  $\Field[x]$. This space is a (commutative) ring.
\end{proposition}

\begin{proof}
  The space is a commutative group with respect to addition. This is so
  because the additive structure is isomorphic to $\F^k$.

  The space is a commutative monoid with respect to multiplication.
  Multiplication is clearly commutative (since it is commutative in
  $\F$). Proving polynomial multiplication is associative takes just a
  little work that I won't bother with. The polynomial 1 is the
  multiplicative identity.

  It is simple to verify that the distributive law holds (again, because
  it already holds in $\F$), so this proves that we have a ring.
\end{proof}

\begin{remark}
  We normally think of polynomials as functions, where $x$ is a
  placeholder for an input to the function. Over $\R$, two polynomials
  are identical if and only if they define the same function.

  This is not necessarily true in other contexts. The reading gives an
  example that both $x, x^2$ define the same function over $\Zmod{2}$,
  even though the functions they define are both the same.

  This emphasizes that maybe we shouldn't think of polynomials quite
  like functions. They are simply expressions in a syntactic sense. The
  notation $\Field[x]$ is really just meaning expressions written in a
  certain form. The $[x]$ part means ``adjoin'' an element to define a
  richer set of expressions. We could write $\Field[x, y]$ to define an
  even fuller space of polynomials with $x^i y^j$ terms.
\end{remark}

\begin{proposition}
  $\F[x]$ is a \define{cancellation ring}: whenever $xy = xz$, we may
  infer $y = z$. However, $\F[x]$ is not a field. A cancellation ring is
  more commonly called an \define{integral domain}.
\end{proposition}

\begin{proof}
  Consider $xy = xz$. Then $x \parens{y - z} = 0$ (distributive law).
  But this clearly only happens if $x = 0$ or $y - z = 0$. So it is safe
  to cancel when we know $x \ne 0$.

  Even though $\F[x]$ is a cancellation ring, it doesn't have inverses.
  Multiplying by a polynomial can never result in a new polynomial of
  lower degree. So \emph{no} polynomial (besides the integers) can have
  an inverse.
\end{proof}

\begin{proposition}
  Any finite cancellation ring $R$ must be a field.
\end{proposition}

\begin{proof}
  For every non-zero element $a$ and pair of distinct elements $b, c$,
  we know $ab \ne ac$. Thus each $ab$ is distinct. There are $\order{R}$
  distinct products, and exactly $\order{R}$ elements in the ring. So
  there exists some $b$ such that $ab = 1$.
\end{proof}

\begin{remark}
  Note this proof doesn't work for \emph{infinite} cancellation rings
  like the integers or polynomials. So even though $\F[x]$ is a
  cancellation ring, we would be wrong to conclude that it must be a
  field.
\end{remark}

\subsection{Polynomial Division and Factorization}

\begin{theorem}
  For any two polynomials $f, g$, there is a unique choice of $q, r$
  such that

  \begin{nedqn}
    f(x)
  \eqcol
    g(x)q(x) + r(x)
  \end{nedqn}

  \noindent
  where $\deg r < \deg g$.
\end{theorem}

\begin{proof}
  We can show this by considering the polynomial long division
  algorithm.
\end{proof}

\begin{definition}
  We may define polynomials \define{modulo} a polynomial $p(x)$ of
  degree $n$. This consists of polynomials with degree less than $n$.
  Addition and multiplication meet the requirements of a ring. Question:
  when are there inverses? We will show this is when $p(x)$ is
  \define{irreducible}.
\end{definition}

\begin{definition}
  Polynomials with leading term 1 are called \define{monic} polynomials.

  If a polynomial cannot be written as the product of two non-constant
  polynomials, then the polynomial is called \define{irreducible}. A
  monic, irreducible polynomial is called a \define{prime polynomial}.
\end{definition}

\begin{proposition}
  Every monic polynomial has a unique factorization into prime
  polynomials.
\end{proposition}

\begin{proof}
  First: note that there is a single monic polynomial of degree zero
  ($f(x) = 1$) and it has a unique factorization. Assume that every
  monic polynomial of degree $n' < n$ has a unique factorization into
  prime polynomials. Take a monic polynomial $f$ with degree $n$. Our
  goal is to show that this, too, has a unique factorization into prime
  polynomials.

  We will show this by contradiction. Assume that $f$ factors into
  $\prod_{i = 1}^{m_p} p_i$ and also $\prod_{j = 1}^{m_q} q_j$. Choose a
  factor with the \emph{lowest degree}. We may assume that this is
  $p_1$.

  It is clear from the factorization of $f$ into $p_i$ that $f$ modulo
  $p_1$ must be zero. Thus the product of $q_j$ modulo $p_1$ must also
  be zero. This sounds suspicious. Let's rewrite each $q_j$ as $a_j p_1
  + b_j$, where $b_j$ must have degree strictly less than $p_1$. Thus:

  \begin{nedqn}
    f
  \equivcol
    0 \pmod{p_1}
  \\
    \prod_j q_j
  \equivcol
    0 \pmod{p_1}
  \\
    \prod_j \parens{a_j p_1 + b_j}
  \equivcol
    0 \pmod{p_1}
  \\
    \prod_j b_j
  \equivcol
    0 \pmod{p_1}
  \intertext{This tells us that there exists a polynomial $c$ such that:}
    \prod_j b_j
  \eqcol
    c p_1
  \end{nedqn}

  Since all $b_j$ have degree strictly less than $p_1$, we know that
  however $c$ might factor into $c_k$, we will have two distinct
  factorizations of the same polynomial: $\prod_j b_j$ and $p_1 \prod_k
  c_k$.

  But note that the degree of $\prod_j b_j$ must be \emph{strictly less}
  than the degree of $\prod_j q_j$. Why? The degree of $p_1$ is never
  greater than the degree of $q_j$. The degree of the remainder $b_j$ is
  always strictly less than $p_1$ and thus also $q_j$.

  We have now found a contradiction to our induction hypothesis, and the
  unique factorization theorem is thus proven.
\end{proof}

\subsection{Constructing of Field of Order $p^k$}

\begin{theorem}
  Polynomials over $\F$ modulo a polynomial $f(x)$ form a field if and
  only if $f(x)$ is irreducible. If $\F$ has order $p$, and $f$ has
  degree $k$, then the order of $\F[x] \bmod f$ is $p^k$.
\end{theorem}

\begin{proof}
  We have already remarked $\F[x] \bmod f$ is a \emph{ring}.

  If $f$ is reducible, note that non-zero factors $f_1, f_2$ multiply to
  $f$. Then we know that $f_1 f_2 \equiv 0 \pmod{f}$. So we know that
  $\F[x] \bmod f$ cannot be a cancellation ring, much less a field.

  So consider any $f$ that is irreducible. Take any element $g \in \F[x]
  \bmod f$. For every other $h \in \F[x] \bmod f$, consider the product
  $g h$. We know these are unique, else $g \parens{h_1 - h_2} \equiv 0
  \pmod{f}$. For distinct $h_1, h_2$, this would factor $f$, which was
  presumed irreducible.
\end{proof}

\begin{theorem}
  If $\F$ is a field, then its multiplicative group $\F^\times$ is
  cyclic.
\end{theorem}

\begin{proof}
  This proof is the same as my proof that $\Zmodp$ is cyclically
  generated. By Lagrange's theorem, we know that every element $x\in\F$
  has order dividing $\order{\F^\times}$. That is:

  \begin{nedqn}
    x^{\order{\F^\times}} - 1
  \eqcol
    0
  \end{nedqn}

  \noindent
  has $\order{\F^\times}$ solutions. Could there be a $d$ properly
  dividing $\order{\F^\times}$ such that always $x^d - 1 = 0$? No,
  because this polynomial can only have at most $d$ roots, by the
  Fundamental Theorem of Algebra. The theorem applies because the
  polynomial is over $\F$ a field.

  We shouldn't exactly jump to conclude that thus there exists an $a$
  with order $\order{\F^\times}$. We should either use a lemma that (1)
  if the exponent of a group is $m$, there exists an element of order
  $m$, OR (2) if there exists no more than one subgroup of order $d$ for
  each $d$ dividing $\order{G}$, then $G$ must be cyclic.

  Anyway, that will let us finally conclude that there does indeed exist
  an $a$ such that $a^d - 1 \ne 0$ for any $d < \order{\F^\times}$. In
  which case $\F^\times$ is cyclically generated by $a$.
\end{proof}

\begin{corollary}
  The polynomial $x^{p^k - 1} - 1$ factors into $\prod_{a\in \F^\times}
  \parens{x - a}$.
\end{corollary}

\subsection{Every Finite Field of Order $p^k$ is Isomorphic To A Quotient Field}

\begin{remark}
  We've shown that for each $\Zmodp$, \emph{if} there is an irreducible
  polynomial $g$ of degree $k$, we can define a polynomial field modulo
  $g$ which is a field of $p^k$ elements.

  Two questions remain: must there always be a degree $k$ irreducible
  polynomial? And must every field of order $p^k$ be isomorphic to such
  a polynomial quotient field (and thus isomorphic to each other).

  I will take a stab at the second question first.
\end{remark}

\begin{lemma}
  Let $\F$ be a field with $p^k$ elements. Then it requires $k$ linearly
  independent $x_0, \ldots, x_{k-1}$ to span all of $\F$.
\end{lemma}

\begin{proof}
  We've previously shown that a a field of $p^k$ elements must have
  additive structure isomorphic to the direct product of $k$ copies of
  $\Zmodp$. This follows from the requirement that the characteristic of
  a finite field must always be prime.

  It follows that $k$ (and no fewer) linearly independent elements of
  $\F$ will span the entire space $\F$.
\end{proof}

\begin{lemma}
  Let $\F$ be a field with $p^k$ elements. Then we can choose $x$ such
  that $x^0, x^1, \ldots, x^{k-1}$ are all linearly independent.
\end{lemma}

\begin{proof}
  Now, we know that there exists some $x^1$ that generates all of
  $\F^\times$ through repeated multiplication (since $\F^\times$ is
  cyclic). Note that $x^1$ cannot be in the span of $x^0 = 1$, since
  this would imply that $x^1$ is an integer. We know that the product of
  integers must remain an integer, and thus $x^1$ could not generate
  $\F^\times$. (Exception: this is fine if $k = 1$.)

  I argue that each successive $x^i$ is linearly independent from the
  preceding choices for $i < k$. If $x^i$ were not linearly independent
  of the preceding choices, then no further power of $x$ can be either.
  The reason is that then the product $x^1 x^i$ could be written as $x^1
  \sum_{j = 0}^{i - 1} a_j x^k$. We could apply the distributive law to
  distribute $x^1$, which gives us $\sum_{j=1}^{i} a_j x^j$. But this
  would remain in the span of the prior $x^0, \ldots, x^{i - 1}$.

  Thus the first $k$ choices $x^0, \ldots, x^{k - 1}$ must all be
  linearly independent. If not, then powers of $x^1$ could never yield a
  set of $k$ linearly independent elements of $\F$ which we know exists.
\end{proof}

\begin{lemma}
  There exists a homomorphism from $\parens{\Zmodp}[x]$ to a finite
  field $\F$ of $p^k$ elements.
\end{lemma}

\begin{proof}
  Our prior lemma has allowed us to write any element of $\F$ as a
  linear combination of the $x^0, \ldots, x^{k-1}$. In other words, we
  can write any element as a polynomial over $\Zmodp$. The usual
  polynomial addition and multiplication operations exactly correspond
  to the addition and multiplication in $\F$. (Note, it matters that we
  chose $x^0 = 1$, so that multiplication of the constant term gave
  another constant!)

  However, there are too many polynomials (a countably infinite number
  of them)! Polynomials with degree greater than $k-1$ are always
  equivalent to some polynomial with degree less than $k$. We know this
  because the first $k$ choices of $x^0, \ldots x^{k-1}$ already span
  all of $\F$.

  In summary: there is a homomorphism from $\parens{\Zmodp}[x]$ and
  $\F$. But it is not an isomorphism, since the homomorphism is not
  injective.
\end{proof}

\begin{lemma}
  There exists an irreducible polynomial $g$ of degree $k$ such that
  $\parens{\Zmodp}[x] \bmod g$ is in isomorphism with a field $\F$ of
  order $p^k$.
\end{lemma}

\begin{proof}
  How can we reduce polynomials with degree greater than $k-1$ to
  equivalent polynomials of degree less than $k$? It suffices to know
  $a_0, \ldots, a_{k-1}$ such that $x^k = \sum_{i=0}^{k-1} a_i x^i$. We
  may thus rewrite a degree $k$ polynomial as a degree $k-1$ polynomial.
  Likewise we can reduce polynomials of higher degrees step-by-step.

  What we are doing, effectively, is mapping a polynomial $f$ to its
  remainder modulo $g(x) = x^k - \sum_{i=0}^{k-1} a_i x^i$. I claim that
  $g$ must be irreducible. If it were not, there would be two non-zero
  polynomials $f_1, f_2$ such that $f_1 f_2 \equiv 0$. But then there
  would be corresponding non-zero elements of $\F$ that multiply to
  zero. We know that would violate invertibility.

  Thus we know the finite field $\F$ is isomorphic to a quotient field
  $\parens{\Zmodp}[x] \bmod g$ for some irreducible $g$.
\end{proof}
